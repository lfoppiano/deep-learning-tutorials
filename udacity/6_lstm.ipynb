{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MvEblsgEXxrd",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KyVd8FxT5QBc",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Q5rxZK6RDuGe",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Tanh_1:0\", shape=(64, 64), dtype=float32)\n",
      "Tensor(\"Sigmoid_2:0\", shape=(64, 64), dtype=float32)\n",
      "Tensor(\"Tanh_4:0\", shape=(64, 64), dtype=float32)\n",
      "Tensor(\"Sigmoid_5:0\", shape=(64, 64), dtype=float32)\n",
      "Tensor(\"Tanh_7:0\", shape=(64, 64), dtype=float32)\n",
      "Tensor(\"Sigmoid_8:0\", shape=(64, 64), dtype=float32)\n",
      "Tensor(\"Tanh_10:0\", shape=(64, 64), dtype=float32)\n",
      "Tensor(\"Sigmoid_11:0\", shape=(64, 64), dtype=float32)\n",
      "Tensor(\"Tanh_13:0\", shape=(64, 64), dtype=float32)\n",
      "Tensor(\"Sigmoid_14:0\", shape=(64, 64), dtype=float32)\n",
      "Tensor(\"Tanh_16:0\", shape=(64, 64), dtype=float32)\n",
      "Tensor(\"Sigmoid_17:0\", shape=(64, 64), dtype=float32)\n",
      "Tensor(\"Tanh_19:0\", shape=(64, 64), dtype=float32)\n",
      "Tensor(\"Sigmoid_20:0\", shape=(64, 64), dtype=float32)\n",
      "Tensor(\"Tanh_22:0\", shape=(64, 64), dtype=float32)\n",
      "Tensor(\"Sigmoid_23:0\", shape=(64, 64), dtype=float32)\n",
      "Tensor(\"Tanh_25:0\", shape=(64, 64), dtype=float32)\n",
      "Tensor(\"Sigmoid_26:0\", shape=(64, 64), dtype=float32)\n",
      "Tensor(\"Tanh_28:0\", shape=(64, 64), dtype=float32)\n",
      "Tensor(\"Sigmoid_29:0\", shape=(64, 64), dtype=float32)\n",
      "Tensor(\"Tanh_31:0\", shape=(1, 64), dtype=float32)\n",
      "Tensor(\"Sigmoid_32:0\", shape=(1, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf  \n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    print(tf.tanh(state))\n",
    "    print(output_gate)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = []\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = []\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294629 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.97\n",
      "================================================================================\n",
      "p hhkanxktvmczaoi mqn  fnp nlcxoym rdnlxqievshm dlflil xf x t hnyrvoehdsvuptzdwt\n",
      "ypjurt a hrmhslkwtevxtrs yaspaeat njfvl exorsreijjf i wxkzwront xc f eevdelrcsri\n",
      "hw yjsnepvaaax  bipe wlavphif mbrastidr imojttcyv toz fz nj eieicoc ez tjdaxsgma\n",
      "ghnqvaaomlnbua ykv an ssr huib iwi ral rcnezhrrcapfo yzaq iaudtzndiiajzyn fyjakw\n",
      "mjtprggdzavulxh ftfu perfev a csif jsl   eneaukqcftyufbcle eq ure so yfo emmddcm\n",
      "================================================================================\n",
      "Validation set perplexity: 20.30\n",
      "Average loss at step 100: 2.591063 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.80\n",
      "Validation set perplexity: 10.24\n",
      "Average loss at step 200: 2.242414 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.59\n",
      "Validation set perplexity: 8.48\n",
      "Average loss at step 300: 2.099504 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.45\n",
      "Validation set perplexity: 7.98\n",
      "Average loss at step 400: 2.000159 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.44\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 500: 1.930816 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 600: 1.907092 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 700: 1.857930 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 800: 1.816139 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 6.19\n",
      "Average loss at step 900: 1.823143 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 1000: 1.819091 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "================================================================================\n",
      "t ditren endice rimorged to clest x eculs comproudd collecies the lutormal disio\n",
      "lisled of for exelllries prosile of kial one nine six overs four with yelar of k\n",
      "ust be of the morther five eight seven two the sixt of the castuint of unkraf nu\n",
      "ation liphally uastom and devoun it to the compinilat wis replon at two zero zer\n",
      "s byciate produgication plrerey in the approferiin goin buras oov bay apsident g\n",
      "================================================================================\n",
      "Validation set perplexity: 5.93\n",
      "Average loss at step 1100: 1.772792 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 1200: 1.749893 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 1300: 1.730416 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1400: 1.743627 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1500: 1.730681 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 1600: 1.743047 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 1700: 1.707235 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 1800: 1.673032 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 1900: 1.645344 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2000: 1.696144 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "================================================================================\n",
      "qually to bla beincons on not only a plates it orkadic the antimestor vering ats\n",
      "may indiced motision to the lewns in a vimb working borch rifts on theremon addi\n",
      "hia rpringing world beliging forter inspiring pridand in fameing tam in a toxwsa\n",
      "re  its somens contuachics treved of soccollwas langus and finst also proviqee t\n",
      "plef sporican alcy out tognlating was batis of a including this concirig thaten \n",
      "================================================================================\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2100: 1.683789 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 2200: 1.677414 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 2300: 1.637382 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 2400: 1.658640 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2500: 1.678631 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 2600: 1.655208 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 2700: 1.657640 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 2800: 1.650489 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 2900: 1.649755 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3000: 1.646500 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "jisnied buts and stulled a huck and stacity daporfically fast crosk kwowsh only \n",
      "land asceatics gamaning doidis experiatural portacia lincolly of he had i of toi\n",
      "chance the are leyso dered others inststan its is also recondy and diraturich fr\n",
      "ly und they formon one six gordes hole four zero eder by one a wisounism game ot\n",
      "s trandon gere was internation with handoril notar the plake his cest that falat\n",
      "================================================================================\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 3100: 1.631175 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3200: 1.648017 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3300: 1.639331 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3400: 1.667893 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3500: 1.657888 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3600: 1.666623 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 3700: 1.647182 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3800: 1.641761 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3900: 1.637393 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 4000: 1.651951 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "================================================================================\n",
      "me as also one eight b one eight four two one right close in stucting relation a\n",
      "ters amues in the from generation s reporzai tons haddofi moners whice the indic\n",
      "ine rive orspater dis oreg underver in the developh beatthreses opealablar s ars\n",
      "fest ackeed team in some mark s information or manally attonied doight of first \n",
      "um enisionific revelop be u terks used the ease by liw to hemper incomes a jepyr\n",
      "================================================================================\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4100: 1.632811 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4200: 1.634931 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4300: 1.617501 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4400: 1.610783 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 4500: 1.615706 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4600: 1.613012 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4700: 1.626063 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4800: 1.629761 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 4900: 1.629347 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5000: 1.603555 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "================================================================================\n",
      "h pagiete wolitisnand seevan occupitor brite calmony qian a levidies hott two tw\n",
      "tureinknent day of decoolding had by the bosition gapbes in ofted layter umberly\n",
      "ulting the final that caproesly the ordeps rembe and in prefifien is abogd paych\n",
      "invon by spoch cata his crimase uniment of the garcles such mounth of the haves \n",
      "werence s is are of the sox the sciencua dewine of howesa veaged daus matire as \n",
      "================================================================================\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5100: 1.605179 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5200: 1.593517 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5300: 1.572751 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5400: 1.575170 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5500: 1.564605 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5600: 1.577491 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5700: 1.562567 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5800: 1.578595 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5900: 1.571442 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6000: 1.545333 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "ine from economics west a miroms in misable of east of a souther typical terms a\n",
      "x overshernow but issoliss and starks and the ramon to polinies in thable is tru\n",
      "n ceryman and that feft ganaid rives from b in one two zero amalatly markilitinn\n",
      "boshum serukers dapon statedy and universoveio in traditions as a be lidet links\n",
      "ur population whened formary of jockland camountocated traditions formean maulbs\n",
      "================================================================================\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6100: 1.566811 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6200: 1.535118 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6300: 1.546355 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6400: 1.545073 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6500: 1.555967 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6600: 1.592641 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6700: 1.578610 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6800: 1.601352 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6900: 1.579878 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 7000: 1.576661 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "================================================================================\n",
      "phen dates unutish upard eaplogiles this now them martovi ages inforuagually lap\n",
      "orly nears eepate on cocj or filcher amginer respute president or and stringral \n",
      "mstaring fout the rava owned is he haw be perian countions techning hare who are\n",
      "reto warered tons a fur scoppert is gighfese wus slaginged to and heldgy or age \n",
      "k dock the ortuction bite butsherm of they witho sext works the order az takes o\n",
      "================================================================================\n",
      "Validation set perplexity: 4.23\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = {}\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "  data_x = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes * 4], -0.1, 0.1))   \n",
    "  data_m = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))   \n",
    "  data_b = tf.Variable(tf.truncated_normal([1, num_nodes * 4], -0.1, 0.1))   \n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf  \n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    \n",
    "    general = tf.matmul(i, data_x) + tf.matmul(o, data_m) + data_b\n",
    "\n",
    "    input_gate = tf.sigmoid(tf.slice(general, [0,0], [-1,num_nodes]))\n",
    "    forget_gate = tf.sigmoid(tf.slice(general, [0,num_nodes], [-1,num_nodes]))    \n",
    "    update = tf.sigmoid(tf.slice(general, [0,2*num_nodes], [-1,num_nodes]))  \n",
    "    output_gate = tf.sigmoid(tf.slice(general, [0,3*num_nodes], [-1,num_nodes]))\n",
    "\n",
    "#     input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "#     forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "#     update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "#     output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = []\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = []\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.290447 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.85\n",
      "================================================================================\n",
      "m zdxe ykldt ethnp editbem e oilpzhjjkvcnjq ois whaetimtik mwaos d d nsbiqlejtwx\n",
      "dv ywi lzwue rzih hkviivegyeda llb sb eot r maguuraosdxyd cneicx cm wfpq  a elmt\n",
      "meiomnll ekoidldorkefue   evyl cebto krqejyb qpsoii hwlezo mkt dgn filawdonoicps\n",
      "p  wejlbnumpmwuhjg ai zeaipacfresueasigwelxbahi  pf weeldww t ceeubos g ogod   u\n",
      " nsyqzmi alxballm ovecaoiznueinpta hrd anik sc nx ddgfas amgqywdyaoxqt kpcbo lo \n",
      "================================================================================\n",
      "Validation set perplexity: 19.83\n",
      "Average loss at step 100: 2.844453 learning rate: 10.000000\n",
      "Minibatch perplexity: 17.30\n",
      "Validation set perplexity: 17.43\n",
      "Average loss at step 200: 2.655156 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.79\n",
      "Validation set perplexity: 12.45\n",
      "Average loss at step 300: 2.497663 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.51\n",
      "Validation set perplexity: 11.14\n",
      "Average loss at step 400: 2.416421 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.36\n",
      "Validation set perplexity: 10.87\n",
      "Average loss at step 500: 2.354205 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.44\n",
      "Validation set perplexity: 10.88\n",
      "Average loss at step 600: 2.343152 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.59\n",
      "Validation set perplexity: 10.28\n",
      "Average loss at step 700: 2.289486 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.63\n",
      "Validation set perplexity: 10.50\n",
      "Average loss at step 800: 2.247538 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.35\n",
      "Validation set perplexity: 9.57\n",
      "Average loss at step 900: 2.225720 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.74\n",
      "Validation set perplexity: 9.35\n",
      "Average loss at step 1000: 2.209136 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.96\n",
      "================================================================================\n",
      "gice sonoon any rich pine went bee nuved sives ancer be ulrackicole farc the oni\n",
      "y memand pived slon axt fcf utrein sope e vo of an poum eigh up in keal turesian\n",
      "ine mucanlize vo ussaulen eitaling bum ink ish biugter ouy zatus on nnoy frot as\n",
      "le rieve frent of jople bevanat thet of ine me onis orf weredar ie froof ucee pr\n",
      "pe your the fy of wort fetl tho of the of bure wace ens ur wero tilt do s it if \n",
      "================================================================================\n",
      "Validation set perplexity: 9.57\n",
      "Average loss at step 1100: 2.167450 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.45\n",
      "Validation set perplexity: 9.36\n",
      "Average loss at step 1200: 2.132770 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.80\n",
      "Validation set perplexity: 8.80\n",
      "Average loss at step 1300: 2.108501 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.49\n",
      "Validation set perplexity: 8.69\n",
      "Average loss at step 1400: 2.114428 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.43\n",
      "Validation set perplexity: 8.55\n",
      "Average loss at step 1500: 2.108911 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.65\n",
      "Validation set perplexity: 8.56\n",
      "Average loss at step 1600: 2.086350 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.72\n",
      "Validation set perplexity: 8.51\n",
      "Average loss at step 1700: 2.075414 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.04\n",
      "Validation set perplexity: 8.33\n",
      "Average loss at step 1800: 2.029052 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.13\n",
      "Validation set perplexity: 8.85\n",
      "Average loss at step 1900: 2.002615 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.42\n",
      "Validation set perplexity: 8.57\n",
      "Average loss at step 2000: 2.043930 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.11\n",
      "================================================================================\n",
      "juus is passeed inurf washived imblears agdiex concled nopmandind jophosed lodd \n",
      "forstived forcs one sesty solliand in dsing bevings britiguence ariames masear h\n",
      "zer one seblut natiased sircs daperll ind of pionsts zengrearions are cumpous so\n",
      "e adsiond mogiep orded kndile prior of dabmodlitictionsing gandirassuss frony a \n",
      "wirs diest semat in insealan fisiodh rishopall wondd prigrdald rectimived hat wh\n",
      "================================================================================\n",
      "Validation set perplexity: 8.33\n",
      "Average loss at step 2100: 2.045402 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.23\n",
      "Validation set perplexity: 7.92\n",
      "Average loss at step 2200: 2.032406 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.72\n",
      "Validation set perplexity: 7.90\n",
      "Average loss at step 2300: 1.987303 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.95\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 2400: 1.997183 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 2500: 1.996527 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.74\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 2600: 1.976055 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.68\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 2700: 1.982551 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.64\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 2800: 1.967928 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.70\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 2900: 1.958756 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.36\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 3000: 1.949658 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.75\n",
      "================================================================================\n",
      "xopime soroxs of somaltive carl in to seis indershew the colliants ut orould of \n",
      "noln wowichis yechisting a hreen and ass mante is of worricule frewancets mosie \n",
      " fric dlecanal the carms of the plays wam oce spopluberabs cancenventy dey eqan \n",
      "rexips oure hey the inante preight are ray laryholobled patite byereleapally a d\n",
      "ise seplean conds hopestantest in the mosly arche coutsion for rucce theide froo\n",
      "================================================================================\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 3100: 1.920609 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.82\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 3200: 1.932459 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.27\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 3300: 1.930516 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.84\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 3400: 1.957612 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.84\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 3500: 1.940491 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.19\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 3600: 1.941425 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 3700: 1.921236 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.62\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 3800: 1.903938 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.05\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 3900: 1.906796 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.75\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 4000: 1.912168 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "================================================================================\n",
      "clibaty prossic the the groedican and covy remeated furisilan pinmed hast to deo\n",
      "aly by or mins fouc a befleaterpes inhoat and storm somal cose pistent othouch t\n",
      "cian it mokais in make poped manl isly termsspaine chese vitre intemak thein oph\n",
      "jomc otal zero setrove from enwuthocates offlamm thistrshammyey ave lown of hawn\n",
      "x zero eide of sex estrork hepcome proestroned asec of the mare and elorcipous f\n",
      "================================================================================\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 4100: 1.896237 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 4200: 1.903631 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.71\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 4300: 1.883205 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 4400: 1.879291 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 4500: 1.866271 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.62\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 4600: 1.864492 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 4700: 1.891780 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.70\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 4800: 1.873103 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 4900: 1.887390 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 5000: 1.838332 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.98\n",
      "================================================================================\n",
      "hass grad diduscical sas nine nine no non tyoner one eight for gard b state novi\n",
      "dirend the two zero zeso ninin a buss one nine nine six zero zero zo zero zerone\n",
      "ralian as that wo kinile bathen from at elabbre of of moy gonish resums his the \n",
      "tiis in two zaries the sedral of formeand jus that whan folld statew s ha contro\n",
      "bomin deferrean ibone romemian wintle was folture it radimpk and and suchs un wr\n",
      "================================================================================\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 5100: 1.845660 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 5200: 1.850028 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 5300: 1.823893 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 5400: 1.824019 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 5500: 1.827992 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 5600: 1.834370 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 5700: 1.823113 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 5800: 1.833695 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 5900: 1.826866 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 6000: 1.800524 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.48\n",
      "================================================================================\n",
      "stal of puctures one six zero zero hin expenal pomou had las one nine five on th\n",
      "fiction chasnusions and a eight the respance o elsusiin in peast avo maye five s\n",
      "beht where term thlet of states other dooks of dinddyunes mandi mevaticula their\n",
      "c selbam that fromed exclysirs pore tolestan and their are eighnu the stayal hav\n",
      "ght that han deotinaric one six eight thores is to unitver relacorg bracks ycloa\n",
      "================================================================================\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 6100: 1.814540 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 6200: 1.790850 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 6300: 1.802051 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 6400: 1.787057 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 6.21\n",
      "Average loss at step 6500: 1.813359 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 6600: 1.843091 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 6.21\n",
      "Average loss at step 6700: 1.830286 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.65\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 6800: 1.851828 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 6.19\n",
      "Average loss at step 6900: 1.827638 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 7000: 1.827855 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.60\n",
      "================================================================================\n",
      "conce to engwas brows two zereings of the cencinenced alsomass coptrattreaally s\n",
      "t unellent i yes by the bolity chactand richosact holists and arbundenmfite acto\n",
      "quus codect fartationy portic singinersporopests fintt retupupanimin teatidial t\n",
      "gerples to amcomm neture prolimber proyoge makuion was the a benon jement pastem\n",
      "hricetirvares whith frmuling the c yesed are rester an which ar hisuce than repo\n",
      "================================================================================\n",
      "Validation set perplexity: 6.20\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = {}\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
