{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression \n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset = 10000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "  tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits)) + 0.005*tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 34.431984\n",
      "Training accuracy: 8.5%\n",
      "Validation accuracy: 9.6%\n",
      "Loss at step 100: 10.591539\n",
      "Training accuracy: 72.2%\n",
      "Validation accuracy: 71.3%\n",
      "Loss at step 200: 6.278162\n",
      "Training accuracy: 76.6%\n",
      "Validation accuracy: 75.0%\n",
      "Loss at step 300: 3.848575\n",
      "Training accuracy: 79.4%\n",
      "Validation accuracy: 76.9%\n",
      "Loss at step 400: 2.471738\n",
      "Training accuracy: 81.4%\n",
      "Validation accuracy: 78.8%\n",
      "Loss at step 500: 1.691110\n",
      "Training accuracy: 83.0%\n",
      "Validation accuracy: 80.1%\n",
      "Loss at step 600: 1.246318\n",
      "Training accuracy: 83.8%\n",
      "Validation accuracy: 81.0%\n",
      "Loss at step 700: 0.991098\n",
      "Training accuracy: 84.6%\n",
      "Validation accuracy: 81.4%\n",
      "Loss at step 800: 0.843680\n",
      "Training accuracy: 84.8%\n",
      "Validation accuracy: 81.9%\n",
      "Test accuracy: 88.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 801\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # This is a one-time operation which ensures the parameters get initialized as\n",
    "  # we described in the graph: random weights for the matrix, zeros for the\n",
    "  # biases. \n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the loss value and the training predictions returned as numpy\n",
    "    # arrays.\n",
    "\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    if (step % 100 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Training accuracy: %.1f%%' % accuracy(\n",
    "        predictions, train_labels[:train_subset, :]))\n",
    "      # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "      # just to get that one numpy array. Note that it recomputes all its graph\n",
    "      # dependencies.\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### light version \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_layers_size = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([hidden_layers_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layers_size]))\n",
    "    hidden_biases = tf.Variable(tf.truncated_normal([hidden_layers_size]))\n",
    "    \n",
    "    logit_hidden = tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases\n",
    "    \n",
    "    logit = tf.matmul(tf.nn.relu(logit_hidden), weights) + biases\n",
    "\n",
    "    # Training computation.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logit)) \\\n",
    "            + 0.01*tf.nn.l2_loss(weights) + 0.01*tf.nn.l2_loss(hidden_weights) \n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logit)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases), weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases), weights) + biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3463.336426\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 27.9%\n",
      "Minibatch loss at step 500: 1153.818237\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 1000: 421.323700\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 1500: 154.846878\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 2000: 57.057449\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 2500: 21.505241\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 3000: 8.318549\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 85.1%\n",
      "Test accuracy: 90.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### heavy version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-3fc509b53cc1>:35: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "hidden_layers_size = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    input_weights = tf.Variable(tf.truncated_normal([image_size * image_size, batch_size]))\n",
    "    input_biases = tf.Variable(tf.truncated_normal([batch_size]))\n",
    "     \n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([batch_size, hidden_layers_size]))\n",
    "    hidden_biases = tf.Variable(tf.truncated_normal([hidden_layers_size]))\n",
    "    \n",
    "    output_weights = tf.Variable(tf.truncated_normal([hidden_layers_size, num_labels]))\n",
    "    output_biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    \n",
    "    def network(input): \n",
    "        logit_input = tf.matmul(input, input_weights) + input_biases\n",
    "        logit_hidden = tf.matmul(tf.nn.relu(logit_input), hidden_weights) + hidden_biases    \n",
    "        logit_output = tf.matmul(logit_hidden, output_weights) + output_biases\n",
    "        \n",
    "        return logit_output + 0.01*tf.nn.l2_loss(input_weights) + 0.01*tf.nn.l2_loss(hidden_weights) \\\n",
    "                + 0.01*tf.nn.l2_loss(output_weights)\n",
    "\n",
    "    # Training computation.\n",
    "    predictions = network(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=predictions))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.0001).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(predictions)\n",
    "    valid_prediction = tf.nn.softmax(network(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(network(tf_test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3270.537109\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 10.9%\n",
      "Minibatch loss at step 500: 504.530029\n",
      "Minibatch accuracy: 66.4%\n",
      "Validation accuracy: 62.2%\n",
      "Minibatch loss at step 1000: 423.991638\n",
      "Minibatch accuracy: 66.4%\n",
      "Validation accuracy: 68.4%\n",
      "Minibatch loss at step 1500: 519.006836\n",
      "Minibatch accuracy: 64.1%\n",
      "Validation accuracy: 70.4%\n",
      "Minibatch loss at step 2000: 278.909546\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 71.7%\n",
      "Minibatch loss at step 2500: 383.797058\n",
      "Minibatch accuracy: 66.4%\n",
      "Validation accuracy: 72.7%\n",
      "Minibatch loss at step 3000: 214.970551\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 73.5%\n",
      "Test accuracy: 80.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 784)\n",
      "(200000, 10)\n",
      "(256, 784)\n",
      "(256, 10)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.shape)\n",
    "print(train_labels.shape)\n",
    "train_dataset1 = train_dataset[:256, :]\n",
    "train_labels1 = train_labels[:256, :]\n",
    "print(train_dataset1.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "hidden_layers_size = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([hidden_layers_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layers_size]))\n",
    "    hidden_biases = tf.Variable(tf.truncated_normal([hidden_layers_size]))\n",
    "    \n",
    "    logit_hidden = tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases\n",
    "    \n",
    "    logit = tf.matmul(tf.nn.relu(logit_hidden), weights) + biases\n",
    "\n",
    "    # Training computation.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logit)) \\\n",
    "            + 0.01*tf.nn.l2_loss(weights) + 0.01*tf.nn.l2_loss(hidden_weights) \n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logit)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases), weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases), weights) + biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3585.023193\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 22.1%\n",
      "Minibatch loss at step 500: 1159.299194\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 57.5%\n",
      "Minibatch loss at step 1000: 426.268768\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 56.9%\n",
      "Minibatch loss at step 1500: 156.739532\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 57.5%\n",
      "Minibatch loss at step 2000: 57.648376\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.6%\n",
      "Minibatch loss at step 2500: 21.250650\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.5%\n",
      "Minibatch loss at step 3000: 7.905726\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.4%\n",
      "Test accuracy: 75.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels1.shape[0] - batch_size)\n",
    "\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset1[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels1[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_layers_size = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([hidden_layers_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layers_size]))\n",
    "    hidden_biases = tf.Variable(tf.truncated_normal([hidden_layers_size]))\n",
    "    \n",
    "    logit_hidden = tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases\n",
    "    \n",
    "    logit = tf.matmul(tf.nn.relu(tf.nn.dropout(logit_hidden, 0.8)), weights) + biases\n",
    "\n",
    "    # Training computation.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logit)) \\\n",
    "            + 0.01*tf.nn.l2_loss(weights) + 0.01*tf.nn.l2_loss(hidden_weights) \n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logit)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases), weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases), weights) + biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3528.815674\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 25.3%\n",
      "Minibatch loss at step 500: 1156.549316\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 1000: 421.678162\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 1500: 154.540054\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 2000: 57.067448\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 2500: 21.497766\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 3000: 8.293703\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 84.8%\n",
      "Test accuracy: 90.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 784)\n",
      "(200000, 10)\n",
      "(256, 784)\n",
      "(200000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.shape)\n",
    "print(train_labels.shape)\n",
    "train_dataset1 = train_dataset[:256, :]\n",
    "train_labels1 = train_labels[:256, :]\n",
    "print(train_dataset1.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "hidden_layers_size = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([hidden_layers_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layers_size]))\n",
    "    hidden_biases = tf.Variable(tf.truncated_normal([hidden_layers_size]))\n",
    "    \n",
    "    logit_hidden = tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases\n",
    "    \n",
    "    logit = tf.matmul(tf.nn.relu(tf.nn.dropout(logit_hidden, 0.8)), weights) + biases\n",
    "\n",
    "    # Training computation.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logit)) \\\n",
    "            + 0.01*tf.nn.l2_loss(weights) + 0.01*tf.nn.l2_loss(hidden_weights) \n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logit)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases), weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases), weights) + biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3554.497314\n",
      "Minibatch accuracy: 3.1%\n",
      "Validation accuracy: 27.0%\n",
      "Minibatch loss at step 500: 1154.585205\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 64.0%\n",
      "Minibatch loss at step 1000: 424.539520\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.1%\n",
      "Minibatch loss at step 1500: 156.103851\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 64.9%\n",
      "Minibatch loss at step 2000: 57.407085\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.0%\n",
      "Minibatch loss at step 2500: 21.156561\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.8%\n",
      "Minibatch loss at step 3000: 7.874798\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Test accuracy: 75.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels1.shape[0] - batch_size)\n",
    "\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset1[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels1[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_layers_size = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([hidden_layers_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layers_size]))\n",
    "    hidden_biases = tf.Variable(tf.truncated_normal([hidden_layers_size]))\n",
    "    \n",
    "    logit_hidden = tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases\n",
    "    \n",
    "    logit = tf.matmul(tf.nn.relu(tf.nn.dropout(logit_hidden, 0.8)), weights) + biases\n",
    "\n",
    "    # Training computation.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logit)) \\\n",
    "            + 0.01*tf.nn.l2_loss(weights) + 0.01*tf.nn.l2_loss(hidden_weights) \n",
    "\n",
    "    # Optimizer.\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "    \n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 500, 0.1)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logit)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases), weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases), weights) + biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "0\n",
      "0\n",
      "Minibatch loss at step 0: 3473.745117\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 34.1%\n",
      "128\n",
      "1\n",
      "256\n",
      "2\n",
      "384\n",
      "3\n",
      "512\n",
      "4\n",
      "640\n",
      "5\n",
      "768\n",
      "6\n",
      "896\n",
      "7\n",
      "1024\n",
      "8\n",
      "1152\n",
      "9\n",
      "1280\n",
      "10\n",
      "1408\n",
      "11\n",
      "1536\n",
      "12\n",
      "1664\n",
      "13\n",
      "1792\n",
      "14\n",
      "1920\n",
      "15\n",
      "2048\n",
      "16\n",
      "2176\n",
      "17\n",
      "2304\n",
      "18\n",
      "2432\n",
      "19\n",
      "2560\n",
      "20\n",
      "2688\n",
      "21\n",
      "2816\n",
      "22\n",
      "2944\n",
      "23\n",
      "3072\n",
      "24\n",
      "3200\n",
      "25\n",
      "3328\n",
      "26\n",
      "3456\n",
      "27\n",
      "3584\n",
      "28\n",
      "3712\n",
      "29\n",
      "3840\n",
      "30\n",
      "3968\n",
      "31\n",
      "4096\n",
      "32\n",
      "4224\n",
      "33\n",
      "4352\n",
      "34\n",
      "4480\n",
      "35\n",
      "4608\n",
      "36\n",
      "4736\n",
      "37\n",
      "4864\n",
      "38\n",
      "4992\n",
      "39\n",
      "5120\n",
      "40\n",
      "5248\n",
      "41\n",
      "5376\n",
      "42\n",
      "5504\n",
      "43\n",
      "5632\n",
      "44\n",
      "5760\n",
      "45\n",
      "5888\n",
      "46\n",
      "6016\n",
      "47\n",
      "6144\n",
      "48\n",
      "6272\n",
      "49\n",
      "6400\n",
      "50\n",
      "6528\n",
      "51\n",
      "6656\n",
      "52\n",
      "6784\n",
      "53\n",
      "6912\n",
      "54\n",
      "7040\n",
      "55\n",
      "7168\n",
      "56\n",
      "7296\n",
      "57\n",
      "7424\n",
      "58\n",
      "7552\n",
      "59\n",
      "7680\n",
      "60\n",
      "7808\n",
      "61\n",
      "7936\n",
      "62\n",
      "8064\n",
      "63\n",
      "8192\n",
      "64\n",
      "8320\n",
      "65\n",
      "8448\n",
      "66\n",
      "8576\n",
      "67\n",
      "8704\n",
      "68\n",
      "8832\n",
      "69\n",
      "8960\n",
      "70\n",
      "9088\n",
      "71\n",
      "9216\n",
      "72\n",
      "9344\n",
      "73\n",
      "9472\n",
      "74\n",
      "9600\n",
      "75\n",
      "9728\n",
      "76\n",
      "9856\n",
      "77\n",
      "9984\n",
      "78\n",
      "10112\n",
      "79\n",
      "10240\n",
      "80\n",
      "10368\n",
      "81\n",
      "10496\n",
      "82\n",
      "10624\n",
      "83\n",
      "10752\n",
      "84\n",
      "10880\n",
      "85\n",
      "11008\n",
      "86\n",
      "11136\n",
      "87\n",
      "11264\n",
      "88\n",
      "11392\n",
      "89\n",
      "11520\n",
      "90\n",
      "11648\n",
      "91\n",
      "11776\n",
      "92\n",
      "11904\n",
      "93\n",
      "12032\n",
      "94\n",
      "12160\n",
      "95\n",
      "12288\n",
      "96\n",
      "12416\n",
      "97\n",
      "12544\n",
      "98\n",
      "12672\n",
      "99\n",
      "12800\n",
      "100\n",
      "12928\n",
      "101\n",
      "13056\n",
      "102\n",
      "13184\n",
      "103\n",
      "13312\n",
      "104\n",
      "13440\n",
      "105\n",
      "13568\n",
      "106\n",
      "13696\n",
      "107\n",
      "13824\n",
      "108\n",
      "13952\n",
      "109\n",
      "14080\n",
      "110\n",
      "14208\n",
      "111\n",
      "14336\n",
      "112\n",
      "14464\n",
      "113\n",
      "14592\n",
      "114\n",
      "14720\n",
      "115\n",
      "14848\n",
      "116\n",
      "14976\n",
      "117\n",
      "15104\n",
      "118\n",
      "15232\n",
      "119\n",
      "15360\n",
      "120\n",
      "15488\n",
      "121\n",
      "15616\n",
      "122\n",
      "15744\n",
      "123\n",
      "15872\n",
      "124\n",
      "16000\n",
      "125\n",
      "16128\n",
      "126\n",
      "16256\n",
      "127\n",
      "16384\n",
      "128\n",
      "16512\n",
      "129\n",
      "16640\n",
      "130\n",
      "16768\n",
      "131\n",
      "16896\n",
      "132\n",
      "17024\n",
      "133\n",
      "17152\n",
      "134\n",
      "17280\n",
      "135\n",
      "17408\n",
      "136\n",
      "17536\n",
      "137\n",
      "17664\n",
      "138\n",
      "17792\n",
      "139\n",
      "17920\n",
      "140\n",
      "18048\n",
      "141\n",
      "18176\n",
      "142\n",
      "18304\n",
      "143\n",
      "18432\n",
      "144\n",
      "18560\n",
      "145\n",
      "18688\n",
      "146\n",
      "18816\n",
      "147\n",
      "18944\n",
      "148\n",
      "19072\n",
      "149\n",
      "19200\n",
      "150\n",
      "19328\n",
      "151\n",
      "19456\n",
      "152\n",
      "19584\n",
      "153\n",
      "19712\n",
      "154\n",
      "19840\n",
      "155\n",
      "19968\n",
      "156\n",
      "20096\n",
      "157\n",
      "20224\n",
      "158\n",
      "20352\n",
      "159\n",
      "20480\n",
      "160\n",
      "20608\n",
      "161\n",
      "20736\n",
      "162\n",
      "20864\n",
      "163\n",
      "20992\n",
      "164\n",
      "21120\n",
      "165\n",
      "21248\n",
      "166\n",
      "21376\n",
      "167\n",
      "21504\n",
      "168\n",
      "21632\n",
      "169\n",
      "21760\n",
      "170\n",
      "21888\n",
      "171\n",
      "22016\n",
      "172\n",
      "22144\n",
      "173\n",
      "22272\n",
      "174\n",
      "22400\n",
      "175\n",
      "22528\n",
      "176\n",
      "22656\n",
      "177\n",
      "22784\n",
      "178\n",
      "22912\n",
      "179\n",
      "23040\n",
      "180\n",
      "23168\n",
      "181\n",
      "23296\n",
      "182\n",
      "23424\n",
      "183\n",
      "23552\n",
      "184\n",
      "23680\n",
      "185\n",
      "23808\n",
      "186\n",
      "23936\n",
      "187\n",
      "24064\n",
      "188\n",
      "24192\n",
      "189\n",
      "24320\n",
      "190\n",
      "24448\n",
      "191\n",
      "24576\n",
      "192\n",
      "24704\n",
      "193\n",
      "24832\n",
      "194\n",
      "24960\n",
      "195\n",
      "25088\n",
      "196\n",
      "25216\n",
      "197\n",
      "25344\n",
      "198\n",
      "25472\n",
      "199\n",
      "25600\n",
      "200\n",
      "25728\n",
      "201\n",
      "25856\n",
      "202\n",
      "25984\n",
      "203\n",
      "26112\n",
      "204\n",
      "26240\n",
      "205\n",
      "26368\n",
      "206\n",
      "26496\n",
      "207\n",
      "26624\n",
      "208\n",
      "26752\n",
      "209\n",
      "26880\n",
      "210\n",
      "27008\n",
      "211\n",
      "27136\n",
      "212\n",
      "27264\n",
      "213\n",
      "27392\n",
      "214\n",
      "27520\n",
      "215\n",
      "27648\n",
      "216\n",
      "27776\n",
      "217\n",
      "27904\n",
      "218\n",
      "28032\n",
      "219\n",
      "28160\n",
      "220\n",
      "28288\n",
      "221\n",
      "28416\n",
      "222\n",
      "28544\n",
      "223\n",
      "28672\n",
      "224\n",
      "28800\n",
      "225\n",
      "28928\n",
      "226\n",
      "29056\n",
      "227\n",
      "29184\n",
      "228\n",
      "29312\n",
      "229\n",
      "29440\n",
      "230\n",
      "29568\n",
      "231\n",
      "29696\n",
      "232\n",
      "29824\n",
      "233\n",
      "29952\n",
      "234\n",
      "30080\n",
      "235\n",
      "30208\n",
      "236\n",
      "30336\n",
      "237\n",
      "30464\n",
      "238\n",
      "30592\n",
      "239\n",
      "30720\n",
      "240\n",
      "30848\n",
      "241\n",
      "30976\n",
      "242\n",
      "31104\n",
      "243\n",
      "31232\n",
      "244\n",
      "31360\n",
      "245\n",
      "31488\n",
      "246\n",
      "31616\n",
      "247\n",
      "31744\n",
      "248\n",
      "31872\n",
      "249\n",
      "32000\n",
      "250\n",
      "32128\n",
      "251\n",
      "32256\n",
      "252\n",
      "32384\n",
      "253\n",
      "32512\n",
      "254\n",
      "32640\n",
      "255\n",
      "32768\n",
      "256\n",
      "32896\n",
      "257\n",
      "33024\n",
      "258\n",
      "33152\n",
      "259\n",
      "33280\n",
      "260\n",
      "33408\n",
      "261\n",
      "33536\n",
      "262\n",
      "33664\n",
      "263\n",
      "33792\n",
      "264\n",
      "33920\n",
      "265\n",
      "34048\n",
      "266\n",
      "34176\n",
      "267\n",
      "34304\n",
      "268\n",
      "34432\n",
      "269\n",
      "34560\n",
      "270\n",
      "34688\n",
      "271\n",
      "34816\n",
      "272\n",
      "34944\n",
      "273\n",
      "35072\n",
      "274\n",
      "35200\n",
      "275\n",
      "35328\n",
      "276\n",
      "35456\n",
      "277\n",
      "35584\n",
      "278\n",
      "35712\n",
      "279\n",
      "35840\n",
      "280\n",
      "35968\n",
      "281\n",
      "36096\n",
      "282\n",
      "36224\n",
      "283\n",
      "36352\n",
      "284\n",
      "36480\n",
      "285\n",
      "36608\n",
      "286\n",
      "36736\n",
      "287\n",
      "36864\n",
      "288\n",
      "36992\n",
      "289\n",
      "37120\n",
      "290\n",
      "37248\n",
      "291\n",
      "37376\n",
      "292\n",
      "37504\n",
      "293\n",
      "37632\n",
      "294\n",
      "37760\n",
      "295\n",
      "37888\n",
      "296\n",
      "38016\n",
      "297\n",
      "38144\n",
      "298\n",
      "38272\n",
      "299\n",
      "38400\n",
      "300\n",
      "38528\n",
      "301\n",
      "38656\n",
      "302\n",
      "38784\n",
      "303\n",
      "38912\n",
      "304\n",
      "39040\n",
      "305\n",
      "39168\n",
      "306\n",
      "39296\n",
      "307\n",
      "39424\n",
      "308\n",
      "39552\n",
      "309\n",
      "39680\n",
      "310\n",
      "39808\n",
      "311\n",
      "39936\n",
      "312\n",
      "40064\n",
      "313\n",
      "40192\n",
      "314\n",
      "40320\n",
      "315\n",
      "40448\n",
      "316\n",
      "40576\n",
      "317\n",
      "40704\n",
      "318\n",
      "40832\n",
      "319\n",
      "40960\n",
      "320\n",
      "41088\n",
      "321\n",
      "41216\n",
      "322\n",
      "41344\n",
      "323\n",
      "41472\n",
      "324\n",
      "41600\n",
      "325\n",
      "41728\n",
      "326\n",
      "41856\n",
      "327\n",
      "41984\n",
      "328\n",
      "42112\n",
      "329\n",
      "42240\n",
      "330\n",
      "42368\n",
      "331\n",
      "42496\n",
      "332\n",
      "42624\n",
      "333\n",
      "42752\n",
      "334\n",
      "42880\n",
      "335\n",
      "43008\n",
      "336\n",
      "43136\n",
      "337\n",
      "43264\n",
      "338\n",
      "43392\n",
      "339\n",
      "43520\n",
      "340\n",
      "43648\n",
      "341\n",
      "43776\n",
      "342\n",
      "43904\n",
      "343\n",
      "44032\n",
      "344\n",
      "44160\n",
      "345\n",
      "44288\n",
      "346\n",
      "44416\n",
      "347\n",
      "44544\n",
      "348\n",
      "44672\n",
      "349\n",
      "44800\n",
      "350\n",
      "44928\n",
      "351\n",
      "45056\n",
      "352\n",
      "45184\n",
      "353\n",
      "45312\n",
      "354\n",
      "45440\n",
      "355\n",
      "45568\n",
      "356\n",
      "45696\n",
      "357\n",
      "45824\n",
      "358\n",
      "45952\n",
      "359\n",
      "46080\n",
      "360\n",
      "46208\n",
      "361\n",
      "46336\n",
      "362\n",
      "46464\n",
      "363\n",
      "46592\n",
      "364\n",
      "46720\n",
      "365\n",
      "46848\n",
      "366\n",
      "46976\n",
      "367\n",
      "47104\n",
      "368\n",
      "47232\n",
      "369\n",
      "47360\n",
      "370\n",
      "47488\n",
      "371\n",
      "47616\n",
      "372\n",
      "47744\n",
      "373\n",
      "47872\n",
      "374\n",
      "48000\n",
      "375\n",
      "48128\n",
      "376\n",
      "48256\n",
      "377\n",
      "48384\n",
      "378\n",
      "48512\n",
      "379\n",
      "48640\n",
      "380\n",
      "48768\n",
      "381\n",
      "48896\n",
      "382\n",
      "49024\n",
      "383\n",
      "49152\n",
      "384\n",
      "49280\n",
      "385\n",
      "49408\n",
      "386\n",
      "49536\n",
      "387\n",
      "49664\n",
      "388\n",
      "49792\n",
      "389\n",
      "49920\n",
      "390\n",
      "50048\n",
      "391\n",
      "50176\n",
      "392\n",
      "50304\n",
      "393\n",
      "50432\n",
      "394\n",
      "50560\n",
      "395\n",
      "50688\n",
      "396\n",
      "50816\n",
      "397\n",
      "50944\n",
      "398\n",
      "51072\n",
      "399\n",
      "51200\n",
      "400\n",
      "51328\n",
      "401\n",
      "51456\n",
      "402\n",
      "51584\n",
      "403\n",
      "51712\n",
      "404\n",
      "51840\n",
      "405\n",
      "51968\n",
      "406\n",
      "52096\n",
      "407\n",
      "52224\n",
      "408\n",
      "52352\n",
      "409\n",
      "52480\n",
      "410\n",
      "52608\n",
      "411\n",
      "52736\n",
      "412\n",
      "52864\n",
      "413\n",
      "52992\n",
      "414\n",
      "53120\n",
      "415\n",
      "53248\n",
      "416\n",
      "53376\n",
      "417\n",
      "53504\n",
      "418\n",
      "53632\n",
      "419\n",
      "53760\n",
      "420\n",
      "53888\n",
      "421\n",
      "54016\n",
      "422\n",
      "54144\n",
      "423\n",
      "54272\n",
      "424\n",
      "54400\n",
      "425\n",
      "54528\n",
      "426\n",
      "54656\n",
      "427\n",
      "54784\n",
      "428\n",
      "54912\n",
      "429\n",
      "55040\n",
      "430\n",
      "55168\n",
      "431\n",
      "55296\n",
      "432\n",
      "55424\n",
      "433\n",
      "55552\n",
      "434\n",
      "55680\n",
      "435\n",
      "55808\n",
      "436\n",
      "55936\n",
      "437\n",
      "56064\n",
      "438\n",
      "56192\n",
      "439\n",
      "56320\n",
      "440\n",
      "56448\n",
      "441\n",
      "56576\n",
      "442\n",
      "56704\n",
      "443\n",
      "56832\n",
      "444\n",
      "56960\n",
      "445\n",
      "57088\n",
      "446\n",
      "57216\n",
      "447\n",
      "57344\n",
      "448\n",
      "57472\n",
      "449\n",
      "57600\n",
      "450\n",
      "57728\n",
      "451\n",
      "57856\n",
      "452\n",
      "57984\n",
      "453\n",
      "58112\n",
      "454\n",
      "58240\n",
      "455\n",
      "58368\n",
      "456\n",
      "58496\n",
      "457\n",
      "58624\n",
      "458\n",
      "58752\n",
      "459\n",
      "58880\n",
      "460\n",
      "59008\n",
      "461\n",
      "59136\n",
      "462\n",
      "59264\n",
      "463\n",
      "59392\n",
      "464\n",
      "59520\n",
      "465\n",
      "59648\n",
      "466\n",
      "59776\n",
      "467\n",
      "59904\n",
      "468\n",
      "60032\n",
      "469\n",
      "60160\n",
      "470\n",
      "60288\n",
      "471\n",
      "60416\n",
      "472\n",
      "60544\n",
      "473\n",
      "60672\n",
      "474\n",
      "60800\n",
      "475\n",
      "60928\n",
      "476\n",
      "61056\n",
      "477\n",
      "61184\n",
      "478\n",
      "61312\n",
      "479\n",
      "61440\n",
      "480\n",
      "61568\n",
      "481\n",
      "61696\n",
      "482\n",
      "61824\n",
      "483\n",
      "61952\n",
      "484\n",
      "62080\n",
      "485\n",
      "62208\n",
      "486\n",
      "62336\n",
      "487\n",
      "62464\n",
      "488\n",
      "62592\n",
      "489\n",
      "62720\n",
      "490\n",
      "62848\n",
      "491\n",
      "62976\n",
      "492\n",
      "63104\n",
      "493\n",
      "63232\n",
      "494\n",
      "63360\n",
      "495\n",
      "63488\n",
      "496\n",
      "63616\n",
      "497\n",
      "63744\n",
      "498\n",
      "63872\n",
      "499\n",
      "64000\n",
      "500\n",
      "Minibatch loss at step 500: 438.379150\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 82.6%\n",
      "64128\n",
      "501\n",
      "64256\n",
      "502\n",
      "64384\n",
      "503\n",
      "64512\n",
      "504\n",
      "64640\n",
      "505\n",
      "64768\n",
      "506\n",
      "64896\n",
      "507\n",
      "65024\n",
      "508\n",
      "65152\n",
      "509\n",
      "65280\n",
      "510\n",
      "65408\n",
      "511\n",
      "65536\n",
      "512\n",
      "65664\n",
      "513\n",
      "65792\n",
      "514\n",
      "65920\n",
      "515\n",
      "66048\n",
      "516\n",
      "66176\n",
      "517\n",
      "66304\n",
      "518\n",
      "66432\n",
      "519\n",
      "66560\n",
      "520\n",
      "66688\n",
      "521\n",
      "66816\n",
      "522\n",
      "66944\n",
      "523\n",
      "67072\n",
      "524\n",
      "67200\n",
      "525\n",
      "67328\n",
      "526\n",
      "67456\n",
      "527\n",
      "67584\n",
      "528\n",
      "67712\n",
      "529\n",
      "67840\n",
      "530\n",
      "67968\n",
      "531\n",
      "68096\n",
      "532\n",
      "68224\n",
      "533\n",
      "68352\n",
      "534\n",
      "68480\n",
      "535\n",
      "68608\n",
      "536\n",
      "68736\n",
      "537\n",
      "68864\n",
      "538\n",
      "68992\n",
      "539\n",
      "69120\n",
      "540\n",
      "69248\n",
      "541\n",
      "69376\n",
      "542\n",
      "69504\n",
      "543\n",
      "69632\n",
      "544\n",
      "69760\n",
      "545\n",
      "69888\n",
      "546\n",
      "70016\n",
      "547\n",
      "70144\n",
      "548\n",
      "70272\n",
      "549\n",
      "70400\n",
      "550\n",
      "70528\n",
      "551\n",
      "70656\n",
      "552\n",
      "70784\n",
      "553\n",
      "70912\n",
      "554\n",
      "71040\n",
      "555\n",
      "71168\n",
      "556\n",
      "71296\n",
      "557\n",
      "71424\n",
      "558\n",
      "71552\n",
      "559\n",
      "71680\n",
      "560\n",
      "71808\n",
      "561\n",
      "71936\n",
      "562\n",
      "72064\n",
      "563\n",
      "72192\n",
      "564\n",
      "72320\n",
      "565\n",
      "72448\n",
      "566\n",
      "72576\n",
      "567\n",
      "72704\n",
      "568\n",
      "72832\n",
      "569\n",
      "72960\n",
      "570\n",
      "73088\n",
      "571\n",
      "73216\n",
      "572\n",
      "73344\n",
      "573\n",
      "73472\n",
      "574\n",
      "73600\n",
      "575\n",
      "73728\n",
      "576\n",
      "73856\n",
      "577\n",
      "73984\n",
      "578\n",
      "74112\n",
      "579\n",
      "74240\n",
      "580\n",
      "74368\n",
      "581\n",
      "74496\n",
      "582\n",
      "74624\n",
      "583\n",
      "74752\n",
      "584\n",
      "74880\n",
      "585\n",
      "75008\n",
      "586\n",
      "75136\n",
      "587\n",
      "75264\n",
      "588\n",
      "75392\n",
      "589\n",
      "75520\n",
      "590\n",
      "75648\n",
      "591\n",
      "75776\n",
      "592\n",
      "75904\n",
      "593\n",
      "76032\n",
      "594\n",
      "76160\n",
      "595\n",
      "76288\n",
      "596\n",
      "76416\n",
      "597\n",
      "76544\n",
      "598\n",
      "76672\n",
      "599\n",
      "76800\n",
      "600\n",
      "76928\n",
      "601\n",
      "77056\n",
      "602\n",
      "77184\n",
      "603\n",
      "77312\n",
      "604\n",
      "77440\n",
      "605\n",
      "77568\n",
      "606\n",
      "77696\n",
      "607\n",
      "77824\n",
      "608\n",
      "77952\n",
      "609\n",
      "78080\n",
      "610\n",
      "78208\n",
      "611\n",
      "78336\n",
      "612\n",
      "78464\n",
      "613\n",
      "78592\n",
      "614\n",
      "78720\n",
      "615\n",
      "78848\n",
      "616\n",
      "78976\n",
      "617\n",
      "79104\n",
      "618\n",
      "79232\n",
      "619\n",
      "79360\n",
      "620\n",
      "79488\n",
      "621\n",
      "79616\n",
      "622\n",
      "79744\n",
      "623\n",
      "79872\n",
      "624\n",
      "80000\n",
      "625\n",
      "80128\n",
      "626\n",
      "80256\n",
      "627\n",
      "80384\n",
      "628\n",
      "80512\n",
      "629\n",
      "80640\n",
      "630\n",
      "80768\n",
      "631\n",
      "80896\n",
      "632\n",
      "81024\n",
      "633\n",
      "81152\n",
      "634\n",
      "81280\n",
      "635\n",
      "81408\n",
      "636\n",
      "81536\n",
      "637\n",
      "81664\n",
      "638\n",
      "81792\n",
      "639\n",
      "81920\n",
      "640\n",
      "82048\n",
      "641\n",
      "82176\n",
      "642\n",
      "82304\n",
      "643\n",
      "82432\n",
      "644\n",
      "82560\n",
      "645\n",
      "82688\n",
      "646\n",
      "82816\n",
      "647\n",
      "82944\n",
      "648\n",
      "83072\n",
      "649\n",
      "83200\n",
      "650\n",
      "83328\n",
      "651\n",
      "83456\n",
      "652\n",
      "83584\n",
      "653\n",
      "83712\n",
      "654\n",
      "83840\n",
      "655\n",
      "83968\n",
      "656\n",
      "84096\n",
      "657\n",
      "84224\n",
      "658\n",
      "84352\n",
      "659\n",
      "84480\n",
      "660\n",
      "84608\n",
      "661\n",
      "84736\n",
      "662\n",
      "84864\n",
      "663\n",
      "84992\n",
      "664\n",
      "85120\n",
      "665\n",
      "85248\n",
      "666\n",
      "85376\n",
      "667\n",
      "85504\n",
      "668\n",
      "85632\n",
      "669\n",
      "85760\n",
      "670\n",
      "85888\n",
      "671\n",
      "86016\n",
      "672\n",
      "86144\n",
      "673\n",
      "86272\n",
      "674\n",
      "86400\n",
      "675\n",
      "86528\n",
      "676\n",
      "86656\n",
      "677\n",
      "86784\n",
      "678\n",
      "86912\n",
      "679\n",
      "87040\n",
      "680\n",
      "87168\n",
      "681\n",
      "87296\n",
      "682\n",
      "87424\n",
      "683\n",
      "87552\n",
      "684\n",
      "87680\n",
      "685\n",
      "87808\n",
      "686\n",
      "87936\n",
      "687\n",
      "88064\n",
      "688\n",
      "88192\n",
      "689\n",
      "88320\n",
      "690\n",
      "88448\n",
      "691\n",
      "88576\n",
      "692\n",
      "88704\n",
      "693\n",
      "88832\n",
      "694\n",
      "88960\n",
      "695\n",
      "89088\n",
      "696\n",
      "89216\n",
      "697\n",
      "89344\n",
      "698\n",
      "89472\n",
      "699\n",
      "89600\n",
      "700\n",
      "89728\n",
      "701\n",
      "89856\n",
      "702\n",
      "89984\n",
      "703\n",
      "90112\n",
      "704\n",
      "90240\n",
      "705\n",
      "90368\n",
      "706\n",
      "90496\n",
      "707\n",
      "90624\n",
      "708\n",
      "90752\n",
      "709\n",
      "90880\n",
      "710\n",
      "91008\n",
      "711\n",
      "91136\n",
      "712\n",
      "91264\n",
      "713\n",
      "91392\n",
      "714\n",
      "91520\n",
      "715\n",
      "91648\n",
      "716\n",
      "91776\n",
      "717\n",
      "91904\n",
      "718\n",
      "92032\n",
      "719\n",
      "92160\n",
      "720\n",
      "92288\n",
      "721\n",
      "92416\n",
      "722\n",
      "92544\n",
      "723\n",
      "92672\n",
      "724\n",
      "92800\n",
      "725\n",
      "92928\n",
      "726\n",
      "93056\n",
      "727\n",
      "93184\n",
      "728\n",
      "93312\n",
      "729\n",
      "93440\n",
      "730\n",
      "93568\n",
      "731\n",
      "93696\n",
      "732\n",
      "93824\n",
      "733\n",
      "93952\n",
      "734\n",
      "94080\n",
      "735\n",
      "94208\n",
      "736\n",
      "94336\n",
      "737\n",
      "94464\n",
      "738\n",
      "94592\n",
      "739\n",
      "94720\n",
      "740\n",
      "94848\n",
      "741\n",
      "94976\n",
      "742\n",
      "95104\n",
      "743\n",
      "95232\n",
      "744\n",
      "95360\n",
      "745\n",
      "95488\n",
      "746\n",
      "95616\n",
      "747\n",
      "95744\n",
      "748\n",
      "95872\n",
      "749\n",
      "96000\n",
      "750\n",
      "96128\n",
      "751\n",
      "96256\n",
      "752\n",
      "96384\n",
      "753\n",
      "96512\n",
      "754\n",
      "96640\n",
      "755\n",
      "96768\n",
      "756\n",
      "96896\n",
      "757\n",
      "97024\n",
      "758\n",
      "97152\n",
      "759\n",
      "97280\n",
      "760\n",
      "97408\n",
      "761\n",
      "97536\n",
      "762\n",
      "97664\n",
      "763\n",
      "97792\n",
      "764\n",
      "97920\n",
      "765\n",
      "98048\n",
      "766\n",
      "98176\n",
      "767\n",
      "98304\n",
      "768\n",
      "98432\n",
      "769\n",
      "98560\n",
      "770\n",
      "98688\n",
      "771\n",
      "98816\n",
      "772\n",
      "98944\n",
      "773\n",
      "99072\n",
      "774\n",
      "99200\n",
      "775\n",
      "99328\n",
      "776\n",
      "99456\n",
      "777\n",
      "99584\n",
      "778\n",
      "99712\n",
      "779\n",
      "99840\n",
      "780\n",
      "99968\n",
      "781\n",
      "100096\n",
      "782\n",
      "100224\n",
      "783\n",
      "100352\n",
      "784\n",
      "100480\n",
      "785\n",
      "100608\n",
      "786\n",
      "100736\n",
      "787\n",
      "100864\n",
      "788\n",
      "100992\n",
      "789\n",
      "101120\n",
      "790\n",
      "101248\n",
      "791\n",
      "101376\n",
      "792\n",
      "101504\n",
      "793\n",
      "101632\n",
      "794\n",
      "101760\n",
      "795\n",
      "101888\n",
      "796\n",
      "102016\n",
      "797\n",
      "102144\n",
      "798\n",
      "102272\n",
      "799\n",
      "102400\n",
      "800\n",
      "102528\n",
      "801\n",
      "102656\n",
      "802\n",
      "102784\n",
      "803\n",
      "102912\n",
      "804\n",
      "103040\n",
      "805\n",
      "103168\n",
      "806\n",
      "103296\n",
      "807\n",
      "103424\n",
      "808\n",
      "103552\n",
      "809\n",
      "103680\n",
      "810\n",
      "103808\n",
      "811\n",
      "103936\n",
      "812\n",
      "104064\n",
      "813\n",
      "104192\n",
      "814\n",
      "104320\n",
      "815\n",
      "104448\n",
      "816\n",
      "104576\n",
      "817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104704\n",
      "818\n",
      "104832\n",
      "819\n",
      "104960\n",
      "820\n",
      "105088\n",
      "821\n",
      "105216\n",
      "822\n",
      "105344\n",
      "823\n",
      "105472\n",
      "824\n",
      "105600\n",
      "825\n",
      "105728\n",
      "826\n",
      "105856\n",
      "827\n",
      "105984\n",
      "828\n",
      "106112\n",
      "829\n",
      "106240\n",
      "830\n",
      "106368\n",
      "831\n",
      "106496\n",
      "832\n",
      "106624\n",
      "833\n",
      "106752\n",
      "834\n",
      "106880\n",
      "835\n",
      "107008\n",
      "836\n",
      "107136\n",
      "837\n",
      "107264\n",
      "838\n",
      "107392\n",
      "839\n",
      "107520\n",
      "840\n",
      "107648\n",
      "841\n",
      "107776\n",
      "842\n",
      "107904\n",
      "843\n",
      "108032\n",
      "844\n",
      "108160\n",
      "845\n",
      "108288\n",
      "846\n",
      "108416\n",
      "847\n",
      "108544\n",
      "848\n",
      "108672\n",
      "849\n",
      "108800\n",
      "850\n",
      "108928\n",
      "851\n",
      "109056\n",
      "852\n",
      "109184\n",
      "853\n",
      "109312\n",
      "854\n",
      "109440\n",
      "855\n",
      "109568\n",
      "856\n",
      "109696\n",
      "857\n",
      "109824\n",
      "858\n",
      "109952\n",
      "859\n",
      "110080\n",
      "860\n",
      "110208\n",
      "861\n",
      "110336\n",
      "862\n",
      "110464\n",
      "863\n",
      "110592\n",
      "864\n",
      "110720\n",
      "865\n",
      "110848\n",
      "866\n",
      "110976\n",
      "867\n",
      "111104\n",
      "868\n",
      "111232\n",
      "869\n",
      "111360\n",
      "870\n",
      "111488\n",
      "871\n",
      "111616\n",
      "872\n",
      "111744\n",
      "873\n",
      "111872\n",
      "874\n",
      "112000\n",
      "875\n",
      "112128\n",
      "876\n",
      "112256\n",
      "877\n",
      "112384\n",
      "878\n",
      "112512\n",
      "879\n",
      "112640\n",
      "880\n",
      "112768\n",
      "881\n",
      "112896\n",
      "882\n",
      "113024\n",
      "883\n",
      "113152\n",
      "884\n",
      "113280\n",
      "885\n",
      "113408\n",
      "886\n",
      "113536\n",
      "887\n",
      "113664\n",
      "888\n",
      "113792\n",
      "889\n",
      "113920\n",
      "890\n",
      "114048\n",
      "891\n",
      "114176\n",
      "892\n",
      "114304\n",
      "893\n",
      "114432\n",
      "894\n",
      "114560\n",
      "895\n",
      "114688\n",
      "896\n",
      "114816\n",
      "897\n",
      "114944\n",
      "898\n",
      "115072\n",
      "899\n",
      "115200\n",
      "900\n",
      "115328\n",
      "901\n",
      "115456\n",
      "902\n",
      "115584\n",
      "903\n",
      "115712\n",
      "904\n",
      "115840\n",
      "905\n",
      "115968\n",
      "906\n",
      "116096\n",
      "907\n",
      "116224\n",
      "908\n",
      "116352\n",
      "909\n",
      "116480\n",
      "910\n",
      "116608\n",
      "911\n",
      "116736\n",
      "912\n",
      "116864\n",
      "913\n",
      "116992\n",
      "914\n",
      "117120\n",
      "915\n",
      "117248\n",
      "916\n",
      "117376\n",
      "917\n",
      "117504\n",
      "918\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-2f5239743ca3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# and the value is the numpy array to feed to it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtf_train_dataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels }\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
