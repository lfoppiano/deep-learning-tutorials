{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression \n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset = 10000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "  tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits)) + 0.005*tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 31.238466\n",
      "Training accuracy: 10.0%\n",
      "Validation accuracy: 12.9%\n",
      "Loss at step 100: 10.268067\n",
      "Training accuracy: 72.7%\n",
      "Validation accuracy: 71.6%\n",
      "Loss at step 200: 6.104387\n",
      "Training accuracy: 76.1%\n",
      "Validation accuracy: 74.5%\n",
      "Loss at step 300: 3.757077\n",
      "Training accuracy: 78.8%\n",
      "Validation accuracy: 76.5%\n",
      "Loss at step 400: 2.424228\n",
      "Training accuracy: 81.0%\n",
      "Validation accuracy: 78.3%\n",
      "Loss at step 500: 1.666318\n",
      "Training accuracy: 82.6%\n",
      "Validation accuracy: 79.6%\n",
      "Loss at step 600: 1.233261\n",
      "Training accuracy: 83.6%\n",
      "Validation accuracy: 80.7%\n",
      "Loss at step 700: 0.984165\n",
      "Training accuracy: 84.2%\n",
      "Validation accuracy: 81.2%\n",
      "Loss at step 800: 0.839969\n",
      "Training accuracy: 84.6%\n",
      "Validation accuracy: 81.7%\n",
      "Test accuracy: 88.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 801\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # This is a one-time operation which ensures the parameters get initialized as\n",
    "  # we described in the graph: random weights for the matrix, zeros for the\n",
    "  # biases. \n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the loss value and the training predictions returned as numpy\n",
    "    # arrays.\n",
    "\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    if (step % 100 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Training accuracy: %.1f%%' % accuracy(\n",
    "        predictions, train_labels[:train_subset, :]))\n",
    "      # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "      # just to get that one numpy array. Note that it recomputes all its graph\n",
    "      # dependencies.\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### light version \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-c67e244a1df7>:27: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "hidden_layers_size = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([hidden_layers_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layers_size]))\n",
    "    hidden_biases = tf.Variable(tf.truncated_normal([hidden_layers_size]))\n",
    "    \n",
    "    logit_hidden = tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases\n",
    "    \n",
    "    logit = tf.matmul(tf.nn.relu(logit_hidden), weights) + biases\n",
    "\n",
    "    # Training computation.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logit)) \\\n",
    "            + 0.01*tf.nn.l2_loss(weights) + 0.01*tf.nn.l2_loss(hidden_weights) \n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logit)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases), weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases), weights) + biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3484.477295\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 24.5%\n",
      "Minibatch loss at step 500: 1152.567139\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 1000: 420.430786\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 1500: 154.686310\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 2000: 56.943432\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 2500: 21.456387\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 3000: 8.307608\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 85.0%\n",
      "Test accuracy: 90.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### heavy version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 4387.492676\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 5.5%\n",
      "Minibatch loss at step 500: 460.699829\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 61.5%\n",
      "Minibatch loss at step 1000: 373.021149\n",
      "Minibatch accuracy: 68.0%\n",
      "Validation accuracy: 67.6%\n",
      "Minibatch loss at step 1500: 446.565002\n",
      "Minibatch accuracy: 68.0%\n",
      "Validation accuracy: 70.1%\n",
      "Minibatch loss at step 2000: 193.107117\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 71.7%\n",
      "Minibatch loss at step 2500: 281.370239\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 72.3%\n",
      "Minibatch loss at step 3000: 224.408997\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 72.9%\n",
      "Minibatch loss at step 3500: 242.409576\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 73.3%\n",
      "Minibatch loss at step 4000: 206.336792\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 74.2%\n",
      "Minibatch loss at step 4500: 100.002655\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 5000: 143.100403\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 74.7%\n",
      "Minibatch loss at step 5500: 194.337128\n",
      "Minibatch accuracy: 68.0%\n",
      "Validation accuracy: 74.8%\n",
      "Minibatch loss at step 6000: 140.121414\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 6500: 114.370621\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 75.1%\n",
      "Minibatch loss at step 7000: 80.624779\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 7500: 76.265015\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 75.5%\n",
      "Minibatch loss at step 8000: 102.054077\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 75.5%\n",
      "Minibatch loss at step 8500: 92.505447\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 9000: 65.130058\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 75.5%\n",
      "Minibatch loss at step 9500: 41.150063\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 10000: 59.508011\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 75.5%\n",
      "Minibatch loss at step 10500: 51.419846\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 75.5%\n",
      "Minibatch loss at step 11000: 28.625343\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 75.5%\n",
      "Minibatch loss at step 11500: 23.242634\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 12000: 27.058558\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 75.3%\n",
      "Minibatch loss at step 12500: 15.534756\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 75.7%\n",
      "Minibatch loss at step 13000: 25.097996\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 75.0%\n",
      "Minibatch loss at step 13500: 13.077888\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 74.2%\n",
      "Minibatch loss at step 14000: 18.280067\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 73.9%\n",
      "Minibatch loss at step 14500: 8.091227\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 73.9%\n",
      "Minibatch loss at step 15000: 11.230005\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 72.0%\n",
      "Minibatch loss at step 15500: 6.924445\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 72.4%\n",
      "Minibatch loss at step 16000: 5.900527\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 73.1%\n",
      "Minibatch loss at step 16500: 5.601096\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 73.1%\n",
      "Minibatch loss at step 17000: 5.728337\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 17500: 3.916914\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 74.7%\n",
      "Minibatch loss at step 18000: 8.652784\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 71.6%\n",
      "Minibatch loss at step 18500: 6.621137\n",
      "Minibatch accuracy: 63.3%\n",
      "Validation accuracy: 67.8%\n",
      "Minibatch loss at step 19000: 7.083324\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 70.9%\n",
      "Minibatch loss at step 19500: 6.977543\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 70.1%\n",
      "Minibatch loss at step 20000: 7.006803\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 73.1%\n",
      "Minibatch loss at step 20500: 4.783115\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 69.7%\n",
      "Minibatch loss at step 21000: 6.069973\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 73.1%\n",
      "Minibatch loss at step 21500: 7.208642\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 22000: 6.194648\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 72.8%\n",
      "Minibatch loss at step 22500: 6.313421\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 73.6%\n",
      "Minibatch loss at step 23000: 5.408141\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 72.4%\n",
      "Minibatch loss at step 23500: 7.012280\n",
      "Minibatch accuracy: 63.3%\n",
      "Validation accuracy: 63.5%\n",
      "Minibatch loss at step 24000: 7.741158\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 69.8%\n",
      "Minibatch loss at step 24500: 10.997772\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 64.5%\n",
      "Minibatch loss at step 25000: 7.952446\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 68.0%\n",
      "Minibatch loss at step 25500: 3.799868\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 68.4%\n",
      "Minibatch loss at step 26000: 3.965933\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 26500: 3.799866\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 27000: 4.582693\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 73.9%\n",
      "Minibatch loss at step 27500: 6.155717\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 72.7%\n",
      "Minibatch loss at step 28000: 4.348281\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 72.3%\n",
      "Minibatch loss at step 28500: 14.012733\n",
      "Minibatch accuracy: 66.4%\n",
      "Validation accuracy: 68.4%\n",
      "Minibatch loss at step 29000: 4.072833\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 68.1%\n",
      "Minibatch loss at step 29500: 5.240834\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 71.0%\n",
      "Minibatch loss at step 30000: 9.378729\n",
      "Minibatch accuracy: 63.3%\n",
      "Validation accuracy: 66.0%\n",
      "Test accuracy: 71.6%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "hidden_layers_size = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    input_weights = tf.Variable(tf.truncated_normal([image_size * image_size, batch_size]))\n",
    "    input_biases = tf.Variable(tf.truncated_normal([batch_size]))\n",
    "     \n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([batch_size, hidden_layers_size]))\n",
    "    hidden_biases = tf.Variable(tf.truncated_normal([hidden_layers_size]))\n",
    "    \n",
    "    output_weights = tf.Variable(tf.truncated_normal([hidden_layers_size, num_labels]))\n",
    "    output_biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    \n",
    "    def network(input): \n",
    "        logit_input = tf.matmul(input, input_weights) + input_biases\n",
    "        logit_hidden = tf.matmul(tf.nn.relu(logit_input), hidden_weights) + hidden_biases    \n",
    "        logit_output = tf.matmul(logit_hidden, output_weights) + output_biases\n",
    "        \n",
    "        return logit_output + 0.01*tf.nn.l2_loss(input_weights) + 0.01*tf.nn.l2_loss(hidden_weights) \\\n",
    "                + 0.01*tf.nn.l2_loss(output_weights)\n",
    "\n",
    "    # Training computation.\n",
    "    predictions = network(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=predictions))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.0001).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(predictions)\n",
    "    valid_prediction = tf.nn.softmax(network(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(network(tf_test_dataset))\n",
    "\n",
    "num_steps = 30001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 784)\n",
      "(200000, 10)\n",
      "(256, 784)\n",
      "(200000, 10)\n",
      "Initialized\n",
      "Minibatch loss at step 0: 3574.677246\n",
      "Minibatch accuracy: 4.7%\n",
      "Validation accuracy: 24.5%\n",
      "Minibatch loss at step 500: 1158.654785\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 55.1%\n",
      "Minibatch loss at step 1000: 426.032745\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 54.7%\n",
      "Minibatch loss at step 1500: 156.652832\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 54.8%\n",
      "Minibatch loss at step 2000: 57.615635\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.2%\n",
      "Minibatch loss at step 2500: 21.237299\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 3000: 7.899362\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.7%\n",
      "Test accuracy: 75.0%\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.shape)\n",
    "print(train_labels.shape)\n",
    "train_dataset1 = train_dataset[:256, :]\n",
    "train_labels1 = train_labels[:256, :]\n",
    "print(train_dataset1.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "hidden_layers_size = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([hidden_layers_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layers_size]))\n",
    "    hidden_biases = tf.Variable(tf.truncated_normal([hidden_layers_size]))\n",
    "    \n",
    "    logit_hidden = tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases\n",
    "    \n",
    "    logit = tf.matmul(tf.nn.relu(logit_hidden), weights) + biases\n",
    "\n",
    "    # Training computation.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logit)) \\\n",
    "            + 0.01*tf.nn.l2_loss(weights) + 0.01*tf.nn.l2_loss(hidden_weights) \n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logit)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases), weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases), weights) + biases)\n",
    "\n",
    "    \n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels1.shape[0] - batch_size)\n",
    "\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset1[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels1[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3619.852051\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 24.2%\n",
      "Minibatch loss at step 500: 1153.475342\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 1000: 418.912567\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 1500: 153.796844\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 2000: 56.797161\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 2500: 21.399641\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 3000: 8.322268\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 84.5%\n",
      "Test accuracy: 90.3%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "hidden_layers_size = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([hidden_layers_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layers_size]))\n",
    "    hidden_biases = tf.Variable(tf.truncated_normal([hidden_layers_size]))\n",
    "    \n",
    "    logit_hidden = tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases\n",
    "    \n",
    "    logit = tf.matmul(tf.nn.dropout(tf.nn.relu(logit_hidden), 0.5), weights) + biases\n",
    "\n",
    "    # Training computation.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logit)) \\\n",
    "            + 0.01*tf.nn.l2_loss(weights) + 0.01*tf.nn.l2_loss(hidden_weights) \n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logit)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases), weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases), weights) + biases)\n",
    "\n",
    "    \n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 784)\n",
      "(200000, 10)\n",
      "(256, 784)\n",
      "(200000, 10)\n",
      "Initialized\n",
      "Minibatch loss at step 0: 3516.660645\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 31.7%\n",
      "Minibatch loss at step 500: 1156.660522\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.7%\n",
      "Minibatch loss at step 1000: 425.300964\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 63.8%\n",
      "Minibatch loss at step 1500: 156.383896\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 63.9%\n",
      "Minibatch loss at step 2000: 57.513275\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.2%\n",
      "Minibatch loss at step 2500: 21.190153\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.8%\n",
      "Minibatch loss at step 3000: 7.892344\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.5%\n",
      "Test accuracy: 75.8%\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.shape)\n",
    "print(train_labels.shape)\n",
    "train_dataset1 = train_dataset[:256, :]\n",
    "train_labels1 = train_labels[:256, :]\n",
    "print(train_dataset1.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "hidden_layers_size = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([hidden_layers_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layers_size]))\n",
    "    hidden_biases = tf.Variable(tf.truncated_normal([hidden_layers_size]))\n",
    "    \n",
    "    logit_hidden = tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases\n",
    "    \n",
    "    logit = tf.matmul(tf.nn.relu(tf.nn.dropout(logit_hidden, 0.8)), weights) + biases\n",
    "\n",
    "    # Training computation.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logit)) \\\n",
    "            + 0.01*tf.nn.l2_loss(weights) + 0.01*tf.nn.l2_loss(hidden_weights) \n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logit)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases), weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases), weights) + biases)\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels1.shape[0] - batch_size)\n",
    "\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset1[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels1[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 91.2%\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "batch_size = 128\n",
    "hidden_layers_size = 1024\n",
    "learning_starting_rate = 0.1\n",
    "learning_decay_steps = 100\n",
    "learning_decay_rate = 0.95\n",
    "beta = 1*10^-5\n",
    "keep_prob_dropouts=0.8\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([hidden_layers_size, num_labels]))\n",
    "#     weights = tf.Variable(tf.truncated_normal([hidden_layers_size, num_labels], stddev=1 / math.sqrt(float(image_size * image_size))))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layers_size]))\n",
    "    hidden_biases = tf.Variable(tf.truncated_normal([hidden_layers_size]))\n",
    "    \n",
    "    logit_hidden = tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases\n",
    "    \n",
    "    logit = tf.matmul(tf.nn.dropout(tf.nn.relu(logit_hidden), keep_prob_dropouts), weights) + biases\n",
    "\n",
    "    # Training computation.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logit)) #\\\n",
    "            #+ beta*tf.nn.l2_loss(weights) + beta*tf.nn.l2_loss(hidden_weights) \n",
    "\n",
    "    # Optimizer.\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "    \n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(learning_starting_rate, global_step, learning_decay_steps, learning_decay_rate)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logit)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases), weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases), weights) + biases)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from IPython import display\n",
    "\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "test_acc_history = []\n",
    "epochs_history = []\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels }\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Offset: %d\" % offset )\n",
    "            print(\"Learning rate: %f\" % lr )            \n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            train_acc = accuracy(predictions, batch_labels)\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % train_acc)\n",
    "            val_acc = accuracy(valid_prediction.eval(), valid_labels)\n",
    "            print(\"Validation accuracy: %.1f%%\" % val_acc)\n",
    "            test_acc = accuracy(test_prediction.eval(), test_labels)\n",
    "\n",
    "            display.clear_output(wait=True)            \n",
    "            epochs_history.append(step)\n",
    "            train_acc_history.append(train_acc)\n",
    "            val_acc_history.append(val_acc)\n",
    "            test_acc_history.append(test_acc)\n",
    "            \n",
    "            def print(x, ys): \n",
    "                for(y in ys): \n",
    "                    plt.plot(epochs_history, train_acc_history, 'r', label='Training accuracy')\n",
    "                    plt.plot(epochs_history, val_acc_history, 'b', label='Validation accuracy')\n",
    "                    plt.plot(epochs_history, test_acc_history, 'g', label='Test accuracy')            \n",
    "                    plt.xlabel('Epochs')\n",
    "                    plt.ylabel('Accuracy')\n",
    "                    plt.legend()\n",
    "            \n",
    "            display.display(plt.show())\n",
    "                     \n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deeper network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl0VPX9//HnW4IS9gSwKFRBsV+FkAkxBJFFEYjSolQQgYILCFQUsXrU0q98D4jVo9aFIi2WUqj+vmigUtxaoKK4HQRZBBQVcUFZNew7Enj//phhvgETMpDchHBfj3PmzMydu7w/hDOvuZ977+eauyMiIuF1WnkXICIi5UtBICIScgoCEZGQUxCIiIScgkBEJOQUBCIiIacgEBEJOQWBiEjIKQhEREIuqbwLSETdunW9UaNG5V2GiEiFsnjx4k3uXq+4+SpEEDRq1IhFixaVdxkiIhWKmX2TyHzqGhIRCTkFgYhIyCkIRERCrkIcIxCRqAMHDrB27Vr27dtX3qXISaRKlSo0bNiQypUrn9DyCgKRCmTt2rXUqFGDRo0aYWblXY6cBNydzZs3s3btWho3bnxC61DXkEgFsm/fPurUqaMQkDgzo06dOiXaS1QQiFQwCgE5Wkn/TygIRERCTkEgIgnbvHkzGRkZZGRkUL9+fRo0aBB//8MPPyS0jv79+7Ny5cpjzvOnP/2JKVOmlEbJkgAdLBaRhNWpU4elS5cCMGrUKKpXr84999xzxDzujrtz2mmF/86cPHlysdu5/fbbS15sGcvPzycpqWJ+pWqPQERK7IsvviAtLY1bb72VzMxMNmzYwODBg8nKyqJZs2aMHj06Pm/btm1ZunQp+fn51K5dm+HDhxOJRGjdujXff/89ACNGjGDMmDHx+YcPH052djb/9V//xbx58wDYvXs3PXr0IBKJ0KdPH7KysuIhVdDIkSNp2bJlvD53B+Dzzz/niiuuIBKJkJmZyerVqwF4+OGHad68OZFIhPvvv/+ImgE2btxIkyZNAJg4cSK9e/ema9eudOnShR07dnDFFVeQmZlJeno6r732WryOyZMnk56eTiQSoX///mzbto3zzjuP/Px8ALZt20bjxo05ePBgqf1dElUx40tE4De/gUK++EokIwNiX8DH65NPPmHy5Mk888wzADzyyCOkpqaSn59Phw4duO6662jatOkRy2zfvp3LLruMRx55hLvvvptJkyYxfPjwH63b3fnggw945ZVXGD16NLNmzeLpp5+mfv36TJ8+nWXLlpGZmVloXXfeeScPPPAA7s6vfvUrZs2aRZcuXejTpw+jRo3i6quvZt++fRw6dIhXX32VmTNn8sEHH5CcnMyWLVuKbff777/P0qVLSUlJ4cCBA7z88svUqFGD77//njZt2tC1a1eWLVvGo48+yrx580hNTWXLli3Url2bNm3aMGvWLLp27crzzz/P9ddfT6VKlU7gX79ktEcgIqXi/PPPp2XLlvH3L7zwApmZmWRmZvLpp5/yySef/GiZ5ORkunTpAsDFF18c/1V+tO7du/9onvfee4/evXsDEIlEaNasWaHLvvHGG2RnZxOJRHj77bdZsWIFW7duZdOmTVx99dVA9IKsqlWrMmfOHAYMGEBycjIAqampxbY7JyeHlJQUIBpYv/3tb0lPTycnJ4c1a9awadMm3nzzTXr16hVf3+HngQMHxrvKJk+eTP/+/YvdXhC0RyBSUZ3gL/egVKtWLf561apV/PGPf+SDDz6gdu3a9OvXr9Dz3E8//fT460qVKsW7SY52xhln/Giew108x7Jnzx6GDh3KkiVLaNCgASNGjIjXUdgpl+5e6PSkpCQOHToE8KN2FGz3c889x/bt21myZAlJSUk0bNiQffv2Fbneyy67jKFDhzJ37lwqV67MhRdeWGybgqA9AhEpdTt27KBGjRrUrFmTDRs2MHv27FLfRtu2bZk2bRoAH330UaF7HHv37uW0006jbt267Ny5k+nTpwOQkpJC3bp1efXVV4Hol/uePXvIycnhb3/7G3v37gWIdw01atSIxYsXA/Diiy8WWdP27ds588wzSUpK4vXXX2fdunUAdOrUidzc3Pj6CnY59evXj759+5bb3gAoCEQkAJmZmTRt2pS0tDQGDRpEmzZtSn0bd9xxB+vWrSM9PZ0nnniCtLQ0atWqdcQ8derU4aabbiItLY1rr72WVq1axT+bMmUKTzzxBOnp6bRt25a8vDy6du3KVVddRVZWFhkZGTz11FMA3Hvvvfzxj3/k0ksvZevWrUXWdMMNNzBv3jyysrL4xz/+wQUXXABAeno69913H+3btycjI4N77703vkzfvn3Zvn07vXr1Ks1/nuNiiexelbesrCzXjWlE4NNPP+Wiiy4q7zJOCvn5+eTn51OlShVWrVpFTk4Oq1atqnCncObm5jJ79uyETqs9lsL+b5jZYnfPKm7ZivUvJiISs2vXLjp27Eh+fj7uzl/+8pcKFwJDhgxhzpw5zJo1q1zrqFj/aiIiMbVr147321dU48ePL+8SAB0jEBEJPQWBiEjIKQhEREJOQSAiEnIKAhFJ2OWXX/6ji8PGjBnDbbfddszlqlevDsD69eu57rrrilx3caeJjxkzhj179sTf//znP2fbtm2JlC7HoCAQkYT16dOH3NzcI6bl5ubSp0+fhJY/++yzj3llbnGODoJ///vf1K5d+4TXV9bcPT5UxclEQSAiCbvuuut47bXX2L9/PwCrV69m/fr1tG3bNn5ef2ZmJs2bN+fll1/+0fKrV68mLS0NiA7/0Lt3b9LT0+nVq1d8WAeInl9/eAjrkSNHAjB27FjWr19Phw4d6NChAxAd+mHTpk0APPnkk6SlpZGWlhYfwnr16tVcdNFFDBo0iGbNmpGTk3PEdg579dVXadWqFS1atKBTp0589913QPRahf79+9O8eXPS09PjQ1TMmjWLzMxMIpEIHTt2BKL3Z3j88cfj60xLS2P16tXxGm677TYyMzNZs2ZNoe0DWLhwIZdeeimRSITs7Gx27txJu3btjhheu02bNixfvvy4/m7F0XUEIhVUeYxCXadOHbKzs5k1axbdunUjNzeXXr16YWZUqVKFGTNmULNmTTZt2sQll1zCNddcU+T9dMePH0/VqlVZvnw5y5cvP2IY6YceeojU1FQOHjxIx44dWb58OcOGDePJJ59k7ty51K1b94h1LV68mMmTJ7NgwQLcnVatWnHZZZeRkpLCqlWreOGFF/jrX//K9ddfz/Tp0+nXr98Ry7dt25b58+djZkycOJHHHnuMJ554ggcffJBatWrx0UcfAbB161by8vIYNGgQ77zzDo0bN05oqOqVK1cyefJk/vznPxfZvgsvvJBevXoxdepUWrZsyY4dO0hOTmbgwIH8/e9/Z8yYMXz++efs37+f9PT0Yrd5PLRHICLHpWD3UMFuIXfnv//7v0lPT6dTp06sW7cu/su6MO+88078Czk9Pf2IL7dp06aRmZlJixYtWLFiRaEDyhX03nvvce2111KtWjWqV69O9+7deffddwFo3LgxGRkZQNFDXa9du5Yrr7yS5s2b84c//IEVK1YAMGfOnCPulpaSksL8+fNp3749jRs3BhIbqvrcc8/lkksuOWb7Vq5cyVlnnRUfyrtmzZokJSXRs2dPXnvtNQ4cOMCkSZO4+eabi93e8Qp0j8DM7gQGAQb81d3HmFkqMBVoBKwGrnf3okdxEpFCldco1L/85S+5++67WbJkCXv37o3/kp8yZQp5eXksXryYypUr06hRo0KHni6osL2Fr7/+mscff5yFCxeSkpLCzTffXOx6jjVm2uEhrCE6jHVhXUN33HEHd999N9dccw1vvfUWo0aNiq/36BoTGaoajhyuuuBQ1UW1r6j1Vq1alc6dO/Pyyy8zbdq0Yg+on4jA9gjMLI1oCGQDEaCrmV0ADAfecPcLgDdi70WkgqhevTqXX345AwYMOOIg8eEhmCtXrszcuXP55ptvjrme9u3bx29Q//HHH8f7vXfs2EG1atWoVasW3333HTNnzowvU6NGDXbu3Fnoul566SX27NnD7t27mTFjBu3atUu4Tdu3b6dBgwYAPPvss/HpOTk5jBs3Lv5+69attG7dmrfffpuvv/4aOHKo6iVLlgCwZMmS+OdHK6p9F154IevXr2fhwoUA7Ny5M37vhYEDBzJs2DBatmyZ0B7I8Qqya+giYL6773H3fOBt4FqgG3D4X/pZ4JcB1iAiAejTpw/Lli2L3yEMosMpL1q0iKysLKZMmVLsTVaGDBnCrl27SE9P57HHHiM7OxuI3m2sRYsWNGvWjAEDBhwxhPXgwYPp0qVL/GDxYZmZmdx8881kZ2fTqlUrBg4cSIsWLRJuz6hRo+jZsyft2rU74vjDiBEj2Lp1K2lpaUQiEebOnUu9evWYMGEC3bt3JxKJxIeP7tGjB1u2bCEjI4Px48fzs5/9rNBtFdW+008/nalTp3LHHXcQiUTo3LlzfK/i4osvpmbNmoHdsyCwYajN7CLgZaA1sJfor/9FwA3uXrvAfFvdPeVY69Iw1CJRGoY6nNavX8/ll1/OZ599xmmnFf77vSTDUAe2R+DunwKPAq8Ds4BlQOH3oSuEmQ02s0VmtigvLy+gKkVETm7PPfccrVq14qGHHioyBEoq0LOG3P1v7p7p7u2BLcAq4DszOwsg9vx9EctOcPcsd8+qV69ekGWKiJy0brzxRtasWUPPnj0D20agQWBmZ8aezwG6Ay8ArwA3xWa5iWj3kYiIlJOgLyibbmZ1gAPA7e6+1cweAaaZ2S3At0BwMSciIsUKNAjc/Ufnb7n7ZqBjkNsVEZHE6cpiEZGQUxCISMI2b95MRkYGGRkZ1K9fnwYNGsTf//DDDwmvZ9KkSWzcuDHASuV4aNA5EUlYnTp14iNhjho1iurVq3PPPfcc93omTZpEZmYm9evXL+0SE5afn09Skr4CQXsEIlJKnn32WbKzs8nIyOC2227j0KFD5Ofnc8MNN9C8eXPS0tIYO3YsU6dOZenSpfTq1avQPYlnnnmGli1bEolE6NmzZ3xsoI0bN9KtWzfS09OJRCIsWLAAgMmTJ8enHb7ytl+/frz00kvxdR6+Mc6cOXPo1KkTvXv3jl95fPXVV3PxxRfTrFkzJk6cGF/mX//6V3yo6ZycHA4ePEiTJk3iQ0ocPHiQ8847L6HRR092ikORCuo3s37D0o2lOw51Rv0Mxlx1/KPZffzxx8yYMYN58+aRlJTE4MGDyc3N5fzzz2fTpk3xYZy3bdtG7dq1efrppxk3blx8VNCCevbsya233grA8OHD+fvf/86QIUO4/fbb6dy5M0OHDiU/P589e/awbNkyHn30UebNm0dqampCX8rz58/nk08+4ZxzzgGiAZaamsqePXvIysqiR48e7N+/nyFDhvDuu+9y7rnnsmXLFipVqkSfPn14/vnnGTp0KLNnzw5s7J+ypj0CESmxOXPmsHDhQrKyssjIyODtt9/myy+/pEmTJqxcuZI777yT2bNnU6tWrWLXtXz5ctq1a0fz5s3Jzc2NDwn91ltv8etf/xqIjvRZs2ZN3nzzTXr16hX/Mk7kS7l169bxEAB46qmniEQitG7dmrVr1/Lll1/y/vvv06FDB84999wj1nvLLbfEB6WbNGlSYGP/lDXtEYhUUCfyyz0o7s6AAQN48MEHf/TZ8uXLmTlzJmPHjmX69OlMmDDhmOu68cYbmTlzJmlpaUycOJH58+fHPzuRIaEPHjwYH8UTjhwSes6cObzzzjvMnz+f5ORk2rZte8whoRs1akRKSgpz587lww8/JCcn55htqSi0RyAiJdapUyemTZsWv23k5s2b+fbbb8nLy8Pd6dmzJw888EB8mOaihpMG2L17N/Xr1+fAgQM8//zz8ekdOnTgmWeeAaJf7jt27KBTp07k5ubGu4QKDgm9ePFiAGbMmMHBgwcL3db27dtJTU0lOTmZFStWxIeAbtOmDW+++WZ8KO2CXU633HILffv2pXfv3oGN/VPWTo1WiEi5at68OSNHjqRTp06kp6eTk5PDd999x5o1a2jfvj0ZGRkMGjSIhx9+GID+/fszcODAQg8Wjx49muzsbDp37kzTpk3j08eNG8fs2bNp3rw5WVlZfPbZZ6Snp3PffffFt3HvvfcC8Otf/5rXX3+d7Oxsli5desTNaQr6xS9+wZ49e4hEIowePZpWrVoB8JOf/ITx48fTrVs3IpEIffv2jS9z7bXXsn379kDuFFZeAhuGujRpGGqRKA1DXf7mz5/P7373O+bOnVvepRyhJMNQ6xiBiEiCHnroISZMmBC/Z/OpQl1DIiIJuv/++/nmm29o3bp1eZdSqhQEIhVMRejOlbJV0v8TCgKRCqRKlSps3rxZYSBx7s7mzZupUqXKCa9DxwhEKpCGDRuydu1adPtWKahKlSo0bNjwhJdXEIhUIJUrV6Zx48blXYacYtQ1JCIScgoCEZGQUxCIiIScgkBEJOQUBCIiIacgEBEJOQWBiEjIKQhEREJOQSAiEnIKAhGRkFMQiIiEnIJARCTkFAQiIiGnIBARCTkFgYhIyAUaBGZ2l5mtMLOPzewFM6tiZh3NbImZLTWz98ysSZA1iIjIsQUWBGbWABgGZLl7GlAJ6A2MB/q6ewbwPDAiqBpERKR4QXcNJQHJZpYEVAXWAw7UjH1eKzZNRETKSWC3qnT3dWb2OPAtsBf4j7v/x8wGAv82s73ADuCSwpY3s8HAYIBzzjknqDJFREIvyK6hFKAb0Bg4G6hmZv2Au4Cfu3tDYDLwZGHLu/sEd89y96x69eoFVaaISOgF2TXUCfja3fPc/QDwT6ANEHH3BbF5pgKXBliDiIgUI8gg+Ba4xMyqmpkBHYFPgFpm9rPYPJ2BTwOsQUREihHkMYIFZvYisATIBz4EJgBrgelmdgjYCgwIqgYRESleYEEA4O4jgZFHTZ4Re4iIyElAVxaLiIScgkBEJOQUBCIiIacgEBEJOQWBiEjIKQhEREJOQSAiEnIKAhGRkFMQiIiEXLFBYGZDYyOJiojIKSiRPYL6wEIzm2ZmV8UGkBMRkVNEsUHg7iOAC4C/ATcDq8zsYTM7P+DaRESkDCR0jMDdHdgYe+QDKcCLZvZYgLWJiEgZKHb0UTMbBtwEbAImAve6+wEzOw1YBdwXbIkiIhKkRIahrgt0d/dvCk5090Nm1jWYskREpKwk0jX0b2DL4TdmVsPMWgG4u+4uJiJSwSUSBOOBXQXe745NExGRU0AiQWCxg8VAtEuIgO9sJiIiZSeRIPjKzIaZWeXY407gq6ALExGRspFIENwKXAqsI3rj+VbA4CCLEhGRslNsF4+7fw/0LoNaRESkHCRyHUEV4BagGVDl8HR3HxBgXSIiUkYS6Rr6f0THG7oSeBtoCOwMsigRESk7iQRBE3f/H2C3uz8L/AJoHmxZIiJSVhIJggOx521mlgbUAhoFVpGIiJSpRK4HmBC7H8EI4BWgOvA/gVYlIiJl5phBEBtYboe7bwXeAc4rk6pERKTMHLNrKHYV8dAyqkVERMpBIscIXjeze8zsp2aWevgReGUiIlImEjlGcPh6gdsLTHPUTSQickpI5MrixmVRiIiIlI9Eriy+sbDp7v5cAsveBQwkugfxEdAf2A/8HugJHATGu/vY46hZRERKUSJdQy0LvK4CdASWAMcMAjNrAAwDmrr7XjObRnTMIgN+ClwYu8vZmSdUuYiIlIpEuobuKPjezGoRHXYi0fUnm9kBoCqwnujewK9iZyQdHtRORETKSSJnDR1tD3BBcTO5+zrgceBbYAOw3d3/A5wP9DKzRWY208wKXZeZDY7NsygvL+8EyhQRkUQkcozgVaJ9/BANjqbAtASWSwG6AY2BbcA/zKwfcAawz92zzKw7MAlod/Ty7j4BmACQlZXlR38uIiKlI5FjBI8XeJ0PfOPuaxNYrhPwtbvnAZjZP4ne4GYtMD02zwxgcuLliohIaUskCL4FNrj7PgAzSzazRu6+OoHlLjGzqsBeogeZFwE7gCuI7glcBnx+grWLiEgpSCQI/kH0l/xhB2PTWhY+e5S7LzCzF4meYZQPfEi0qycZmBI7tXQX0dNLRUSknCQSBEnu/sPhN+7+g5mdnsjK3X0kMPKoyfuJ3tNAREROAomcNZRnZtccfmNm3YBNwZUkIiJlKZE9gluJduWMi71fCxR6tbGIiFQ8iVxQ9iXRg77VAXN33a9YROQUUmzXkJk9bGa13X2Xu+80sxQz+31ZFCciIsFL5BhBF3ffdvhN7G5lPw+uJBERKUuJBEElMzvj8BszSyZ6dbCIiJwCEjlY/L/AG2Z2+Arg/sCzwZUkIiJlKZGDxY+Z2XKiQ0YYMAs4N+jCRESkbCQ6+uhG4BDQg+hQEZ8GVpGIiJSpIvcIzOxnRG8k0wfYDEwlevpohzKqTUREysCxuoY+A94Frnb3LyB+60kRETmFHKtrqAfRLqG5ZvZXM+tI9BiBiIicQooMAnef4e69gAuBt4C7gJ+Y2Xgzyymj+kREJGDFHix2993uPsXduwINgaXA8MArExGRMnFc9yx29y3u/hd3vyKogkREpGydyM3rRUTkFKIgEBEJOQWBiEjIKQhEREJOQSAiEnIKAhGRkFMQiIiEnIJARCTkFAQiIiGnIBARCTkFgYhIyCkIRERCTkEgIhJyCgIRkZBTEIiIhFygQWBmd5nZCjP72MxeMLMqBT572sx2Bbl9EREpXmBBYGYNgGFAlrunAZWA3rHPsoDaQW1bREQSF3TXUBKQbGZJQFVgvZlVAv4A3BfwtkVEJAGBBYG7rwMeB74FNgDb3f0/wFDgFXffENS2RUQkcUF2DaUA3YDGwNlANTO7EegJPJ3A8oPNbJGZLcrLywuqTBGR0Auya6gT8LW757n7AeCfwANAE+ALM1sNVDWzLwpb2N0nuHuWu2fVq1cvwDJFRMItyCD4FrjEzKqamQEdgSfdvb67N3L3RsAed28SYA0iIlKMII8RLABeBJYAH8W2NSGo7YmIyIlJCnLl7j4SGHmMz6sHuX0RESmeriwWEQk5BYGISMgpCEREQk5BICIScgoCEZGQUxCIiIScgkBEJOQUBCIiIacgEBEJOQWBiEjIKQhEREJOQSAiEnIKAhGRkFMQiIiEnIJARCTkFAQiIiGnIBARCTkFgYhIyCkIRERCTkEgIhJyCgIRkZBTEIiIhJyCQEQk5BQEIiIhpyAQEQk5BYGISMgpCEREQk5BICIScgoCEZGQUxCIiIScgkBEJOQCDQIzu8vMVpjZx2b2gplVMbMpZrYyNm2SmVUOsgYRETm2wILAzBoAw4Asd08DKgG9gSnAhUBzIBkYGFQNIiJSvKQyWH+ymR0AqgLr3f0/hz80sw+AhgHXICIixxDYHoG7rwMeB74FNgDbjwqBysANwKygahARkeIF2TWUAnQDGgNnA9XMrF+BWf4MvOPu7xax/GAzW2Rmi/Ly8oIqU0Qk9II8WNwJ+Nrd89z9APBP4FIAMxsJ1APuLmphd5/g7lnunlWvXr0AyxQRCbcgjxF8C1xiZlWBvUBHYJGZDQSuBDq6+6EAty8iIgkILAjcfYGZvQgsAfKBD4EJwG7gG+B9MwP4p7uPDqoOERE5tkDPGnL3kcDIstymiIgcH11ZLCIScgoCEZGQUxCIiIScgkBEJOQUBCIiIacgEBEJOQWBiEjIKQhEREJOQSAiEnIKAhGRkFMQiIiEnIJARCTkFAQiIiGnIBARCTkFgYhIyCkIRERCTkEgIhJyCgIRkZBTEIiIhJyCQEQk5Mzdy7uGYplZHvBNeddxnOoCm8q7iDKmNoeD2lxxnOvu9YqbqUIEQUVkZovcPau86yhLanM4qM2nHnUNiYiEnIJARCTkFATBmVDeBZQDtTkc1OZTjI4RiIiEnPYIRERCTkFQAmaWamavm9mq2HNKEfPdFJtnlZndVMjnr5jZx8FXXHIlabOZVTWzf5nZZ2a2wsweKdvqj4+ZXWVmK83sCzMbXsjnZ5jZ1NjnC8ysUYHPfhebvtLMrizLukviRNtsZp3NbLGZfRR7vqKsaz9RJfk7xz4/x8x2mdk9ZVVzqXN3PU7wATwGDI+9Hg48Wsg8qcBXseeU2OuUAp93B54HPi7v9gTdZqAq0CE2z+nAu0CX8m5TEe2sBHwJnBerdRnQ9Kh5bgOeib3uDUyNvW4am/8MoHFsPZXKu00Bt7kFcHbsdRqwrrzbE3SbC3w+HfgHcE95t+dEH9ojKJluwLOx188CvyxkniuB1919i7tvBV4HrgIws+rA3cDvy6DW0nLCbXb3Pe4+F8DdfwCWAA3LoOYTkQ184e5fxWrNJdr2ggr+W7wIdDQzi03Pdff97v418EVsfSe7E26zu3/o7utj01cAVczsjDKpumRK8nfGzH5J9IfOijKqNxAKgpL5ibtvAIg9n1nIPA2ANQXer41NA3gQeALYE2SRpaykbQbAzGoDVwNvBFRnSRXbhoLzuHs+sB2ok+CyJ6OStLmgHsCH7r4/oDpL0wm32cyqAb8FHiiDOgOVVN4FnOzMbA5Qv5CP7k90FYVMczPLAJq4+11H9zmWt6DaXGD9ScALwFh3/+r4KywTx2xDMfMksuzJqCRtjn5o1gx4FMgpxbqCVJI2PwA85e67YjsIFZaCoBju3qmoz8zsOzM7y903mNlZwPeFzLYWuLzA+4bAW0Br4GIzW03073Cmmb3l7pdTzgJs82ETgFXuPqYUyg3KWuCnBd43BNYXMc/aWLjVArYkuOzJqCRtxswaAjOAG939y+DLLRUlaXMr4DozewyoDRwys33uPi74sktZeR+kqMgP4A8ceeD0sULmSQW+JnqwNCX2OvWoeRpRcQ4Wl6jNRI+HTAdOK++2FNPOJKJ9v435v4OIzY6a53aOPIg4Lfa6GUceLP6KinGwuCRtrh2bv0d5t6Os2nzUPKOowAeLy72Aivwg2jf6BrAq9nz4yy4LmFhgvgFEDxh+AfQvZD0VKQhOuM1Ef2058CmwNPYYWN5tOkZbfw58TvSskvtj00YD18ReVyF6tsgXwAfAeQWWvT+23EpO0jOjSrN5MZYMAAACE0lEQVTNwAhgd4G/61LgzPJuT9B/5wLrqNBBoCuLRURCTmcNiYiEnIJARCTkFAQiIiGnIBARCTkFgYhIyCkIJLTM7KCZLS3w+NHIkyVYd6OKMqKsiK4sljDb6+4Z5V2ESHnTHoHIUcxstZk9amYfxB5NYtPPNbM3zGx57Pmc2PSfmNkMM1sWe1waW1UlM/tr7N4L/zGz5Nj8w8zsk9h6csupmSJxCgIJs+SjuoZ6Ffhsh7tnA+OAw2MijQOec/d0YAowNjZ9LPC2u0eATP5vSOILgD+5ezNgG9FROSE6NEeL2HpuDapxIonSlcUSWma2y92rFzJ9NXCFu39lZpWBje5ex8w2AWe5+4HY9A3uXtfM8oCGXmDY5diIsq+7+wWx978FKrv7781sFrALeAl4yd13BdxUkWPSHoFI4byI10XNU5iC4/Ef5P+Oyf0C+BNwMbA4NqKlSLlREIgUrleB5/djr+cRHX0SoC/wXuz1G8AQADOrZGY1i1qpmZ0G/NSjd2q7j+ionT/aKxEpS/olImGWbGZLC7yf5e6HTyE9w8wWEP2x1Cc2bRgwyczuBfKA/rHpdwITzOwWor/8hwAbithmJeB/zawW0RuePOXu20qtRSInQMcIRI4SO0aQ5e6byrsWkbKgriERkZDTHoGISMhpj0BEJOQUBCIiIacgEBEJOQWBiEjIKQhEREJOQSAiEnL/HwoVTBC1M0ACAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 1280\n",
    "hidden_layers_size = 4096\n",
    "\n",
    "learning_starting_rate = 0.01\n",
    "learning_decay_steps = 1000\n",
    "learning_decay_rate = 0.95\n",
    "beta = 1*10^-5\n",
    "keep_prob_dropouts=0.8\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    input_weights = tf.Variable(tf.truncated_normal([image_size * image_size, batch_size]))\n",
    "    input_biases = tf.Variable(tf.truncated_normal([batch_size]))\n",
    "     \n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([batch_size, hidden_layers_size]))\n",
    "    hidden_biases = tf.Variable(tf.truncated_normal([hidden_layers_size]))\n",
    "    \n",
    "    output_weights = tf.Variable(tf.truncated_normal([hidden_layers_size, num_labels]))\n",
    "    output_biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    \n",
    "    def network(input): \n",
    "        logit_input = tf.matmul(input, input_weights) + input_biases\n",
    "        logit_hidden = tf.matmul(tf.nn.dropout(tf.nn.relu(logit_input), keep_prob_dropouts), hidden_weights) + hidden_biases    \n",
    "        logit_output = tf.matmul(logit_hidden, output_weights) + output_biases\n",
    "        \n",
    "        return logit_output #+ 0.01*tf.nn.l2_loss(input_weights) + 0.01*tf.nn.l2_loss(hidden_weights) \\\n",
    "                #+ 0.01*tf.nn.l2_loss(output_weights)\n",
    "\n",
    "    # Training computation.\n",
    "    predictions = network(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=predictions))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(learning_starting_rate, global_step, learning_decay_steps, learning_decay_rate)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(predictions)\n",
    "    valid_prediction = tf.nn.softmax(network(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(network(tf_test_dataset))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from IPython import display\n",
    "\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "test_acc_history = []\n",
    "epochs_history = []\n",
    "    \n",
    "num_steps = 30001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Learning rate: %f\" % lr )       \n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            train_acc = accuracy(predictions, batch_labels)\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % train_acc)\n",
    "            val_acc = accuracy(valid_prediction.eval(), valid_labels)\n",
    "            print(\"Validation accuracy: %.1f%%\" % val_acc)\n",
    "            test_acc = accuracy(test_prediction.eval(), test_labels)\n",
    "            \n",
    "            \n",
    "            display.clear_output(wait=True)            \n",
    "            epochs_history.append(step)\n",
    "            train_acc_history.append(train_acc)\n",
    "            val_acc_history.append(val_acc)\n",
    "            test_acc_history.append(test_acc)\n",
    "            \n",
    "\n",
    "            plt.plot(epochs_history, train_acc_history, 'r', label='Training accuracy')\n",
    "            plt.plot(epochs_history, val_acc_history, 'b', label='Validation accuracy')\n",
    "            plt.plot(epochs_history, test_acc_history, 'g', label='Test accuracy')            \n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.legend()\n",
    "            \n",
    "            display.display(plt.show())\n",
    "\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
