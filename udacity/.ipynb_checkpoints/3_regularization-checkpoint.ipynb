{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression \n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "batch_size = 10000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, [batch_size, image_size * image_size])\n",
    "  tf_train_labels = tf.placeholder(tf.float32, [batch_size, num_labels])\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  tf_regularisation_beta = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits)) + \\\n",
    "        tf_regularisation_beta*tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the accuracy using several values of Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'test accuracy')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_vals = []\n",
    "print('Initialized')\n",
    "for beta in regul_val: \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, tf_regularisation_beta: beta}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "        test_accuracy = accuracy(test_prediction.eval(), test_labels)\n",
    "        accuracy_vals.append(test_accuracy)\n",
    "\n",
    "plt.semilogx(regul_val, accuracy_vals)          \n",
    "plt.xlabel('beta')\n",
    "plt.ylabel('test accuracy')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 20.088163\n",
      "Training accuracy: 9.0%\n",
      "Validation accuracy: 11.8%\n",
      "Loss at step 500: 2.702981\n",
      "Training accuracy: 76.6%\n",
      "Validation accuracy: 76.4%\n",
      "Loss at step 1000: 1.682336\n",
      "Training accuracy: 78.5%\n",
      "Validation accuracy: 79.2%\n",
      "Loss at step 1500: 1.159061\n",
      "Training accuracy: 81.4%\n",
      "Validation accuracy: 80.9%\n",
      "Loss at step 2000: 0.920835\n",
      "Training accuracy: 81.8%\n",
      "Validation accuracy: 82.0%\n",
      "Loss at step 2500: 0.790443\n",
      "Training accuracy: 82.9%\n",
      "Validation accuracy: 82.7%\n",
      "Loss at step 3000: 0.709097\n",
      "Training accuracy: 83.4%\n",
      "Validation accuracy: 82.9%\n",
      "Test accuracy: 89.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "print('Initialized')\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, tf_regularisation_beta: 1e-3}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "        if (step % 500 == 0):\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Training accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 44.246143\n",
      "Minibatch accuracy: 8.9%\n",
      "Validation accuracy: 12.2%\n",
      "Minibatch loss at step 500: 0.862647\n",
      "Minibatch accuracy: 82.6%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 1000: 0.734559\n",
      "Minibatch accuracy: 82.3%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 1500: 0.724856\n",
      "Minibatch accuracy: 82.7%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 2000: 0.721846\n",
      "Minibatch accuracy: 82.7%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 2500: 0.720554\n",
      "Minibatch accuracy: 82.7%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 3000: 0.708209\n",
      "Minibatch accuracy: 83.0%\n",
      "Validation accuracy: 82.9%\n",
      "Test accuracy: 88.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, tf_regularisation_beta: 1e-2}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### light version \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_layers_size = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_regularisation_beta = tf.placeholder(tf.float32)\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([hidden_layers_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layers_size]))\n",
    "    hidden_biases = tf.Variable(tf.truncated_normal([hidden_layers_size]))\n",
    "    \n",
    "    logit_hidden = tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases\n",
    "    \n",
    "    logit = tf.matmul(tf.nn.relu(logit_hidden), weights) + biases\n",
    "\n",
    "    # Training computation.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logit)) \\\n",
    "            + tf_regularisation_beta*tf.nn.l2_loss(weights) + tf_regularisation_beta*tf.nn.l2_loss(hidden_weights) \n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logit)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases), weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases), weights) + biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'test accuracy')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_vals = []\n",
    "print('Initialized')\n",
    "for beta in regul_val: \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, tf_regularisation_beta: beta}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "        test_accuracy = accuracy(test_prediction.eval(), test_labels)\n",
    "        accuracy_vals.append(test_accuracy)\n",
    "\n",
    "plt.semilogx(regul_val, accuracy_vals)          \n",
    "plt.xlabel('beta')\n",
    "plt.ylabel('test accuracy')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00125892541179417\n"
     ]
    }
   ],
   "source": [
    "beta = regul_val[accuracy_vals.index(max(accuracy_vals))]\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 862.159546\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 38.7%\n",
      "Minibatch loss at step 500: 229.924805\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 1000: 100.234222\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 1500: 46.895138\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 2000: 20.832012\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 2500: 9.744510\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 3000: 4.695959\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.7%\n",
      "Test accuracy: 93.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "beta = 0.00158\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, tf_regularisation_beta: beta}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 784)\n",
      "(200000, 10)\n",
      "(256, 784)\n",
      "(200000, 10)\n",
      "Initialized\n",
      "Minibatch loss at step 0: 351.815063\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 36.8%\n",
      "Minibatch loss at step 500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 64.8%\n",
      "Minibatch loss at step 1000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 64.8%\n",
      "Minibatch loss at step 1500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 64.8%\n",
      "Minibatch loss at step 2000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 64.8%\n",
      "Minibatch loss at step 2500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 64.8%\n",
      "Minibatch loss at step 3000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 64.8%\n",
      "Test accuracy: 70.9%\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.shape)\n",
    "print(train_labels.shape)\n",
    "train_dataset1 = train_dataset[:256, :]\n",
    "train_labels1 = train_labels[:256, :]\n",
    "print(train_dataset1.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "hidden_layers_size = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([hidden_layers_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layers_size]))\n",
    "    hidden_biases = tf.Variable(tf.truncated_normal([hidden_layers_size]))\n",
    "    \n",
    "    logit_hidden = tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases\n",
    "    \n",
    "    logit = tf.matmul(tf.nn.relu(logit_hidden), weights) + biases\n",
    "\n",
    "    # Training computation.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logit)) \n",
    "#             + 0.01*tf.nn.l2_loss(weights) + 0.01*tf.nn.l2_loss(hidden_weights) \n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logit)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases), weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases), weights) + biases)\n",
    "\n",
    "    \n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels1.shape[0] - batch_size)\n",
    "\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset1[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels1[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 784)\n",
      "(200000, 10)\n",
      "(256, 784)\n",
      "(200000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.shape)\n",
    "print(train_labels.shape)\n",
    "train_dataset1 = train_dataset[:256, :]\n",
    "train_labels1 = train_labels[:256, :]\n",
    "print(train_dataset1.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "hidden_layers_size = 1024\n",
    "dropout_keep_prob = 0.6\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_dropout_keep_prob = tf.placeholder(tf.float32)\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([hidden_layers_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layers_size]))\n",
    "    hidden_biases = tf.Variable(tf.truncated_normal([hidden_layers_size]))\n",
    "    \n",
    "    logit_hidden = tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases\n",
    "    \n",
    "    logit = tf.matmul(tf.nn.dropout(tf.nn.relu(logit_hidden), tf_dropout_keep_prob), weights) + biases\n",
    "\n",
    "    # Training computation.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logit))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logit)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases), weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases), weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'test accuracy')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "accuracy_vals = []\n",
    "dropout_keep_prob_vals = np.arange(0.0, 0.9, 0.1)\n",
    "print(\"Initialized\")\n",
    "for dropout in dropout_keep_prob_vals: \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels1.shape[0] - batch_size)\n",
    "\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset1[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels1[offset:(offset + batch_size), :]\n",
    "\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, tf_dropout_keep_prob: dropout}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "        test_accuracy = accuracy(test_prediction.eval(), test_labels)\n",
    "        accuracy_vals.append(test_accuracy)\n",
    "\n",
    "# plt.semilogx(dropout_keep_prob_vals, accuracy_vals)    \n",
    "plt.plot(dropout_keep_prob_vals, accuracy_vals)\n",
    "plt.xlabel('beta')\n",
    "plt.ylabel('test accuracy')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    }
   ],
   "source": [
    "beta = dropout_keep_prob_vals[accuracy_vals.index(max(accuracy_vals))]\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 817.704224\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 33.4%\n",
      "Minibatch loss at step 500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.9%\n",
      "Minibatch loss at step 1000: 1.820763\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 69.5%\n",
      "Minibatch loss at step 1500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 2000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.9%\n",
      "Minibatch loss at step 2500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.7%\n",
      "Minibatch loss at step 3000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.1%\n",
      "Test accuracy: 76.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels1.shape[0] - batch_size)\n",
    "\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset1[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels1[offset:(offset + batch_size), :]\n",
    "\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, tf_dropout_keep_prob:0.2}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "batch_size = 128\n",
    "hidden_layers_size = 1024\n",
    "learning_starting_rate = 0.5\n",
    "learning_decay_steps = 1000\n",
    "learning_decay_rate = 0.95\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_regularisation_beta = tf.placeholder(tf.float32)\n",
    "    tf_decay_rate = tf.placeholder(tf.float32)\n",
    "    tf_decay_step = tf.placeholder(tf.float32)\n",
    "    tf_dropout_keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([hidden_layers_size, num_labels]))\n",
    "    biases = tf.Variable(tf.truncated_normal([num_labels]))\n",
    "    \n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layers_size]))\n",
    "    hidden_biases = tf.Variable(tf.truncated_normal([hidden_layers_size]))\n",
    "    \n",
    "    logit_hidden = tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases\n",
    "    \n",
    "    logit = tf.matmul(tf.nn.dropout(tf.nn.relu(logit_hidden), tf_dropout_keep_prob), weights) + biases\n",
    "\n",
    "    # Training computation.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logit) \\\n",
    "            + tf_regularisation_beta * tf.nn.l2_loss(weights) \\\n",
    "            + tf_regularisation_beta * tf.nn.l2_loss(hidden_weights)) \n",
    "    \n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(learning_starting_rate, global_step, tf_decay_step, tf_decay_rate)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logit)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases), weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases), weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finding the best decay rate - for now we don't use the L2 regularisation to have one parameter less to care about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-89-d65129eb65ca>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-89-d65129eb65ca>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    for keep_prob_dropouts in dropout_keep_prob_vals\u001b[0m\n\u001b[0m                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from IPython import display\n",
    "\n",
    "num_steps = 3001\n",
    "beta = 0\n",
    "#beta = 0.00158\n",
    "#keep_prob_dropouts=1\n",
    "\n",
    "dropout_keep_prob_vals = np.arange(0.0, 0.9, 0.1)\n",
    "decay_rate_vals = np.arange(0.10, 1.0, 0.05)\n",
    "decay_steps = [100, 1000, 10000] \n",
    "\n",
    "print(\"Initialized\")\n",
    "\n",
    "for keep_prob_dropouts in dropout_keep_prob_vals:\n",
    "    print(\"dropout keep prob \" + str(keep_prob_dropouts))\n",
    "    for decay_step in decay_steps: \n",
    "        print(\"Decay steps \"+str(decay_step))\n",
    "        test_accuracy_vals = []\n",
    "        for decay_rate in decay_rate_vals: \n",
    "            with tf.Session(graph=graph) as session:\n",
    "                tf.global_variables_initializer().run()\n",
    "\n",
    "                for step in range(num_steps):\n",
    "                    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "                    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "                    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "                    feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, tf_regularisation_beta: beta, tf_dropout_keep_prob: keep_prob_dropouts, tf_decay_rate: decay_rate, tf_decay_step: decay_step}\n",
    "                    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "\n",
    "                test_accuracy = accuracy(test_prediction.eval(), test_labels)\n",
    "                test_accuracy_vals.append(test_accuracy)\n",
    "\n",
    "        plt.plot(decay_rate_vals, test_accuracy_vals, label=\"Decay step \" + str(decay_step))\n",
    "        plt.xlabel('Decay rate')\n",
    "        plt.ylabel('Accuracy')    \n",
    "        plt.legend()\n",
    "\n",
    "        beta = decay_rate_vals[test_accuracy_vals.index(max(test_accuracy_vals))]\n",
    "        print(\"Best value: \" + str(beta) + \" with accuracy: \" + str(max(test_accuracy_vals)))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Learning rate: 0.500000\n",
      "Minibatch loss at step 0: 474.739685\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 28.4%\n",
      "Learning rate: 0.335410\n",
      "Minibatch loss at step 500: 32.849525\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 79.8%\n",
      "Learning rate: 0.225000\n",
      "Minibatch loss at step 1000: 10.840510\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.8%\n",
      "Learning rate: 0.150935\n",
      "Minibatch loss at step 1500: 17.437010\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.5%\n",
      "Learning rate: 0.101250\n",
      "Minibatch loss at step 2000: 5.502504\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 83.3%\n",
      "Learning rate: 0.067921\n",
      "Minibatch loss at step 2500: 3.853841\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 83.4%\n",
      "Learning rate: 0.045562\n",
      "Minibatch loss at step 3000: 4.178963\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 84.3%\n",
      "Learning rate: 0.030564\n",
      "Minibatch loss at step 3500: 2.782155\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 84.3%\n",
      "Learning rate: 0.020503\n",
      "Minibatch loss at step 4000: 2.792761\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 84.6%\n",
      "Learning rate: 0.013754\n",
      "Minibatch loss at step 4500: 3.017077\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 84.7%\n",
      "Learning rate: 0.009226\n",
      "Minibatch loss at step 5000: 1.990372\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 84.8%\n",
      "Learning rate: 0.006189\n",
      "Minibatch loss at step 5500: 2.239071\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 85.1%\n",
      "Learning rate: 0.004152\n",
      "Minibatch loss at step 6000: 4.022453\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 85.2%\n",
      "Test accuracy: 90.8%\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from IPython import display\n",
    "\n",
    "num_steps = 6001\n",
    "beta = 0\n",
    "# beta = 0.00158\n",
    "keep_prob_dropouts=0.6\n",
    "\n",
    "decay_rate = 0.45\n",
    "decay_step = 1000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, tf_regularisation_beta: beta, tf_dropout_keep_prob: keep_prob_dropouts, tf_decay_rate: decay_rate, tf_decay_step: decay_step}\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Learning rate: %f\" % lr )            \n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            train_acc = accuracy(predictions, batch_labels)\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % train_acc)\n",
    "            val_acc = accuracy(valid_prediction.eval(), valid_labels)\n",
    "            print(\"Validation accuracy: %.1f%%\" % val_acc)\n",
    "            test_acc = accuracy(test_prediction.eval(), test_labels)\n",
    "                     \n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deeper network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Learning rate: 0.100000\n",
      "Minibatch loss at step 0: 8.802111\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 22.7%\n",
      "Learning rate: 0.077378\n",
      "Minibatch loss at step 500: 0.795119\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 79.8%\n",
      "Learning rate: 0.059874\n",
      "Minibatch loss at step 1000: 0.645964\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 82.7%\n",
      "Learning rate: 0.046329\n",
      "Minibatch loss at step 1500: 0.685502\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 83.5%\n",
      "Learning rate: 0.035849\n",
      "Minibatch loss at step 2000: 0.407405\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.0%\n",
      "Learning rate: 0.027739\n",
      "Minibatch loss at step 2500: 0.478289\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 84.3%\n",
      "Learning rate: 0.021464\n",
      "Minibatch loss at step 3000: 0.444722\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 84.6%\n",
      "Learning rate: 0.016608\n",
      "Minibatch loss at step 3500: 0.446887\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.1%\n",
      "Learning rate: 0.012851\n",
      "Minibatch loss at step 4000: 0.697524\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 84.9%\n",
      "Learning rate: 0.009944\n",
      "Minibatch loss at step 4500: 0.374864\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.0%\n",
      "Learning rate: 0.007694\n",
      "Minibatch loss at step 5000: 0.491132\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 85.4%\n",
      "Learning rate: 0.005954\n",
      "Minibatch loss at step 5500: 0.590590\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.0%\n",
      "Learning rate: 0.004607\n",
      "Minibatch loss at step 6000: 0.561041\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 85.3%\n",
      "Test accuracy: 91.2%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "hidden_layers_size = 4096\n",
    "\n",
    "learning_starting_rate = 0.1\n",
    "learning_decay_steps = 100\n",
    "learning_decay_rate = 0.95\n",
    "beta = 1*10^-5\n",
    "keep_prob_dropouts=0.7\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    input_weights = tf.Variable(tf.truncated_normal([image_size * image_size, batch_size], mean=0.0, stddev=0.1))\n",
    "    input_biases = tf.Variable(tf.truncated_normal([batch_size]))\n",
    "     \n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([batch_size, hidden_layers_size], mean=0.0, stddev=0.1))\n",
    "    hidden_biases = tf.Variable(tf.truncated_normal([hidden_layers_size]))\n",
    "    \n",
    "    output_weights = tf.Variable(tf.truncated_normal([hidden_layers_size, num_labels], mean=0.0, stddev=0.1))\n",
    "    output_biases = tf.Variable(tf.truncated_normal([num_labels]))\n",
    "\n",
    "    \n",
    "    def network(input): \n",
    "        logit_input = tf.matmul(input, input_weights) + input_biases\n",
    "        logit_hidden = tf.matmul(tf.nn.dropout(tf.nn.relu(logit_input), keep_prob_dropouts), hidden_weights) + hidden_biases    \n",
    "        logit_output = tf.matmul(tf.nn.relu(logit_hidden), output_weights) + output_biases\n",
    "        \n",
    "        return logit_output #+ 0.01*tf.nn.l2_loss(input_weights) + 0.01*tf.nn.l2_loss(hidden_weights) \\\n",
    "                #+ 0.01*tf.nn.l2_loss(output_weights)\n",
    "\n",
    "    # Training computation.\n",
    "    predictions = network(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=predictions))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(learning_starting_rate, global_step, learning_decay_steps, learning_decay_rate)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(predictions)\n",
    "    valid_prediction = tf.nn.softmax(network(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(network(tf_test_dataset))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from IPython import display\n",
    "\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "test_acc_history = []\n",
    "epochs_history = []\n",
    "    \n",
    "num_steps = 6001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Learning rate: %f\" % lr )       \n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            train_acc = accuracy(predictions, batch_labels)\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % train_acc)\n",
    "            val_acc = accuracy(valid_prediction.eval(), valid_labels)\n",
    "            print(\"Validation accuracy: %.1f%%\" % val_acc)\n",
    "            test_acc = accuracy(test_prediction.eval(), test_labels)\n",
    "            \n",
    "            \n",
    "            #display.clear_output(wait=True)            \n",
    "            epochs_history.append(step)\n",
    "            train_acc_history.append(train_acc)\n",
    "            val_acc_history.append(val_acc)\n",
    "            test_acc_history.append(test_acc)\n",
    "            \n",
    "\n",
    "#             plt.plot(epochs_history, train_acc_history, 'r', label='Training accuracy')\n",
    "#             plt.plot(epochs_history, val_acc_history, 'b', label='Validation accuracy')\n",
    "#             plt.plot(epochs_history, test_acc_history, 'g', label='Test accuracy')            \n",
    "#             plt.xlabel('Epochs')\n",
    "#             plt.ylabel('Accuracy')\n",
    "#             plt.legend()\n",
    "            \n",
    "#             plt.show()\n",
    "            #display.display(plt.show())\n",
    "\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
