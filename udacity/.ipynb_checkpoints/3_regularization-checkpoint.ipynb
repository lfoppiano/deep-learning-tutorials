{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression \n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset = 10000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, [batch_size, image_size * image_size])\n",
    "  tf_train_labels = tf.placeholder(tf.float32, [batch_size, num_labels])\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  tf_regularisation_beta = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits)) + \\\n",
    "        tf_regularisation_beta*tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the accuracy using several values of Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'test accuracy')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_vals = []\n",
    "print('Initialized')\n",
    "for beta in regul_val: \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, tf_regularisation_beta: beta}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "        test_accuracy = accuracy(test_prediction.eval(), test_labels)\n",
    "        accuracy_vals.append(test_accuracy)\n",
    "\n",
    "plt.semilogx(regul_val, accuracy_vals)          \n",
    "plt.xlabel('beta')\n",
    "plt.ylabel('test accuracy')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 21.335722\n",
      "Training accuracy: 7.0%\n",
      "Validation accuracy: 11.9%\n",
      "Loss at step 500: 2.602654\n",
      "Training accuracy: 82.0%\n",
      "Validation accuracy: 76.0%\n",
      "Loss at step 1000: 2.105028\n",
      "Training accuracy: 75.8%\n",
      "Validation accuracy: 78.5%\n",
      "Loss at step 1500: 1.505520\n",
      "Training accuracy: 78.9%\n",
      "Validation accuracy: 79.7%\n",
      "Loss at step 2000: 1.034521\n",
      "Training accuracy: 82.8%\n",
      "Validation accuracy: 80.5%\n",
      "Loss at step 2500: 0.980485\n",
      "Training accuracy: 75.8%\n",
      "Validation accuracy: 80.5%\n",
      "Loss at step 3000: 0.727058\n",
      "Training accuracy: 83.6%\n",
      "Validation accuracy: 81.8%\n",
      "Test accuracy: 88.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "print('Initialized')\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, tf_regularisation_beta: 1e-3}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "        if (step % 500 == 0):\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Training accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 47.619568\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 11.9%\n",
      "Minibatch loss at step 500: 0.899004\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 1000: 0.929097\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 1500: 0.966578\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 2000: 0.762910\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 2500: 0.924684\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 80.1%\n",
      "Minibatch loss at step 3000: 0.713308\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 81.6%\n",
      "Test accuracy: 87.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, tf_regularisation_beta: 1e-2}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### light version \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_layers_size = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_regularisation_beta = tf.placeholder(tf.float32)\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([hidden_layers_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layers_size]))\n",
    "    hidden_biases = tf.Variable(tf.truncated_normal([hidden_layers_size]))\n",
    "    \n",
    "    logit_hidden = tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases\n",
    "    \n",
    "    logit = tf.matmul(tf.nn.relu(logit_hidden), weights) + biases\n",
    "\n",
    "    # Training computation.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logit)) \\\n",
    "            + tf_regularisation_beta*tf.nn.l2_loss(weights) + tf_regularisation_beta*tf.nn.l2_loss(hidden_weights) \n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logit)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases), weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases), weights) + biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'test accuracy')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_vals = []\n",
    "print('Initialized')\n",
    "for beta in regul_val: \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, tf_regularisation_beta: beta}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "        test_accuracy = accuracy(test_prediction.eval(), test_labels)\n",
    "        accuracy_vals.append(test_accuracy)\n",
    "\n",
    "plt.semilogx(regul_val, accuracy_vals)          \n",
    "plt.xlabel('beta')\n",
    "plt.ylabel('test accuracy')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0015848931924611173\n"
     ]
    }
   ],
   "source": [
    "beta = regul_val[accuracy_vals.index(max(accuracy_vals))]\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 845.146851\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 30.0%\n",
      "Minibatch loss at step 500: 234.881088\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 79.1%\n",
      "Minibatch loss at step 1000: 101.008453\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 1500: 45.873180\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 2000: 20.826805\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 2500: 9.748866\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 3000: 4.704502\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.4%\n",
      "Test accuracy: 93.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "beta = 0.00158\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, tf_regularisation_beta: beta}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 784)\n",
      "(200000, 10)\n",
      "(256, 784)\n",
      "(200000, 10)\n",
      "Initialized\n",
      "Minibatch loss at step 0: 303.635376\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 33.4%\n",
      "Minibatch loss at step 500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 64.8%\n",
      "Minibatch loss at step 1000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 64.8%\n",
      "Minibatch loss at step 1500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 64.8%\n",
      "Minibatch loss at step 2000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 64.8%\n",
      "Minibatch loss at step 2500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 64.8%\n",
      "Minibatch loss at step 3000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 64.8%\n",
      "Test accuracy: 70.7%\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.shape)\n",
    "print(train_labels.shape)\n",
    "train_dataset1 = train_dataset[:256, :]\n",
    "train_labels1 = train_labels[:256, :]\n",
    "print(train_dataset1.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "hidden_layers_size = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([hidden_layers_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layers_size]))\n",
    "    hidden_biases = tf.Variable(tf.truncated_normal([hidden_layers_size]))\n",
    "    \n",
    "    logit_hidden = tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases\n",
    "    \n",
    "    logit = tf.matmul(tf.nn.relu(logit_hidden), weights) + biases\n",
    "\n",
    "    # Training computation.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logit)) \n",
    "#             + 0.01*tf.nn.l2_loss(weights) + 0.01*tf.nn.l2_loss(hidden_weights) \n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logit)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases), weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases), weights) + biases)\n",
    "\n",
    "    \n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels1.shape[0] - batch_size)\n",
    "\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset1[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels1[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 784)\n",
      "(200000, 10)\n",
      "(256, 784)\n",
      "(200000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.shape)\n",
    "print(train_labels.shape)\n",
    "train_dataset1 = train_dataset[:256, :]\n",
    "train_labels1 = train_labels[:256, :]\n",
    "print(train_dataset1.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "hidden_layers_size = 1024\n",
    "dropout_keep_prob = 0.6\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_dropout_keep_prob = tf.placeholder(tf.float32)\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([hidden_layers_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layers_size]))\n",
    "    hidden_biases = tf.Variable(tf.truncated_normal([hidden_layers_size]))\n",
    "    \n",
    "    logit_hidden = tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases\n",
    "    \n",
    "    logit = tf.matmul(tf.nn.dropout(tf.nn.relu(logit_hidden), tf_dropout_keep_prob), weights) + biases\n",
    "\n",
    "    # Training computation.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logit))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logit)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases), weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases), weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "0.00015848931924611142\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "accuracy_vals = []\n",
    "dropout_keep_prob_vals = np.arange(0.0, 0.9, 0.1)\n",
    "print(\"Initialized\")\n",
    "for dropout in dropout_keep_prob_vals: \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels1.shape[0] - batch_size)\n",
    "\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset1[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels1[offset:(offset + batch_size), :]\n",
    "\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, tf_dropout_keep_prob: dropout}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "        test_accuracy = accuracy(test_prediction.eval(), test_labels)\n",
    "        accuracy_vals.append(test_accuracy)\n",
    "\n",
    "# plt.semilogx(dropout_keep_prob_vals, accuracy_vals)    \n",
    "plt.plot(dropout_keep_prob_vals, accuracy_vals)\n",
    "plt.xlabel('beta')\n",
    "plt.ylabel('test accuracy')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    }
   ],
   "source": [
    "beta = dropout_keep_prob_vals[accuracy_vals.index(max(accuracy_vals))]\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 893.929077\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 32.5%\n",
      "Minibatch loss at step 500: 0.966880\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.1%\n",
      "Minibatch loss at step 1000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.4%\n",
      "Minibatch loss at step 1500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.9%\n",
      "Minibatch loss at step 2000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.4%\n",
      "Minibatch loss at step 2500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.6%\n",
      "Minibatch loss at step 3000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.0%\n",
      "Test accuracy: 76.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels1.shape[0] - batch_size)\n",
    "\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset1[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels1[offset:(offset + batch_size), :]\n",
    "\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, tf_dropout_keep_prob:0.2}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "batch_size = 128\n",
    "hidden_layers_size = 1024\n",
    "learning_starting_rate = 0.5\n",
    "learning_decay_steps = 1000\n",
    "learning_decay_rate = 0.95\n",
    "\n",
    "beta = 0\n",
    "#beta = 0.00158\n",
    "keep_prob_dropouts=0.2\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_regularisation_beta = tf.placeholder(tf.float32)\n",
    "    tf_decay_rate = tf.placeholder(tf.float32)\n",
    "    tf_decay_step = tf.placeholder(tf.float32)\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([hidden_layers_size, num_labels]))\n",
    "    biases = tf.Variable(tf.truncated_normal([num_labels]))\n",
    "    \n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layers_size]))\n",
    "    hidden_biases = tf.Variable(tf.truncated_normal([hidden_layers_size]))\n",
    "    \n",
    "    logit_hidden = tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases\n",
    "    \n",
    "    logit = tf.matmul(tf.nn.dropout(tf.nn.relu(logit_hidden), keep_prob_dropouts), weights) + biases\n",
    "\n",
    "    # Training computation.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logit) \\\n",
    "            + beta * tf.nn.l2_loss(weights) \\\n",
    "            + beta * tf.nn.l2_loss(hidden_weights)) \n",
    "    \n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(learning_starting_rate, global_step, tf_decay_step, tf_decay_rate)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logit)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases), weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases), weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finding the best decay rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from IPython import display\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "decay_rate_vals = np.arange(0.10, 1.0, 0.05)\n",
    "decay_steps = [100, 1000, 10000] \n",
    "\n",
    "print(\"Initialized\")\n",
    "for decay_step in decay_steps: \n",
    "    print(\"Decay steps \"+str(decay_step))\n",
    "    test_accuracy_vals = []\n",
    "    for decay_rate in decay_rate_vals: \n",
    "        with tf.Session(graph=graph) as session:\n",
    "            tf.global_variables_initializer().run()\n",
    "\n",
    "            for step in range(num_steps):\n",
    "                offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "                batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "                batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "                feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, tf_regularisation_beta: beta, tf_decay_rate: decay_rate, tf_decay_step: decay_step}\n",
    "                _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "\n",
    "            test_accuracy = accuracy(test_prediction.eval(), test_labels)\n",
    "            test_accuracy_vals.append(test_accuracy)\n",
    "\n",
    "    plt.plot(decay_rate_vals, test_accuracy_vals, label=\"Decay step \" + str(decay_step))\n",
    "    plt.xlabel('Decay rate')\n",
    "    plt.ylabel('Accuracy')    \n",
    "    plt.legend()\n",
    "    \n",
    "    beta = decay_rate_vals[test_accuracy_vals.index(max(test_accuracy_vals))]\n",
    "    print(\"Best value: \" + str(beta) + \" with accuracy: \" + str(max(test_accuracy_vals)))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decay_rate_vals)\n",
    "print(test_accuracy_vals)\n",
    "\n",
    "plt.plot(decay_rate_vals, test_accuracy_vals, label='Decay steps' + str(decay_step))\n",
    "plt.xlabel('Decay rate')\n",
    "plt.ylabel('Accuracy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from IPython import display\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, tf_regularisation_beta: beta}\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Learning rate: %f\" % lr )            \n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            train_acc = accuracy(predictions, batch_labels)\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % train_acc)\n",
    "            val_acc = accuracy(valid_prediction.eval(), valid_labels)\n",
    "            print(\"Validation accuracy: %.1f%%\" % val_acc)\n",
    "            test_acc = accuracy(test_prediction.eval(), test_labels)\n",
    "\n",
    "#             display.clear_output(wait=True)            \n",
    "#             epochs_history.append(step)\n",
    "#             train_acc_history.append(train_acc)\n",
    "#             val_acc_history.append(val_acc)\n",
    "#             test_acc_history.append(test_acc)\n",
    "            \n",
    "#             def generate_plot(x, ys): \n",
    "#                 for y in ys: \n",
    "#                     plt.plot(epochs_history, train_acc_history, 'r', label='Training accuracy')\n",
    "#                     plt.plot(epochs_history, val_acc_history, 'b', label='Validation accuracy')\n",
    "#                     plt.plot(epochs_history, test_acc_history, 'g', label='Test accuracy')            \n",
    "#                     plt.xlabel('Epochs')\n",
    "#                     plt.ylabel('Accuracy')\n",
    "#                     plt.legend()\n",
    "#                 display.display(plt.show())\n",
    "                     \n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deeper network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Learning rate: 0.100000\n",
      "Minibatch loss at step 0: 7.677100\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 17.2%\n",
      "Learning rate: 0.077378\n",
      "Minibatch loss at step 500: 0.827655\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 80.1%\n",
      "Learning rate: 0.059874\n",
      "Minibatch loss at step 1000: 0.657484\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 83.1%\n",
      "Learning rate: 0.046329\n",
      "Minibatch loss at step 1500: 0.775079\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 82.9%\n",
      "Learning rate: 0.035849\n",
      "Minibatch loss at step 2000: 0.472462\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 84.0%\n",
      "Learning rate: 0.027739\n",
      "Minibatch loss at step 2500: 0.519350\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 84.1%\n",
      "Learning rate: 0.021464\n",
      "Minibatch loss at step 3000: 0.491791\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 84.9%\n",
      "Learning rate: 0.016608\n",
      "Minibatch loss at step 3500: 0.438713\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.9%\n",
      "Learning rate: 0.012851\n",
      "Minibatch loss at step 4000: 0.543776\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 85.3%\n",
      "Learning rate: 0.009944\n",
      "Minibatch loss at step 4500: 0.329333\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 85.3%\n",
      "Learning rate: 0.007694\n",
      "Minibatch loss at step 5000: 0.466251\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 84.8%\n",
      "Learning rate: 0.005954\n",
      "Minibatch loss at step 5500: 0.657532\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.7%\n",
      "Learning rate: 0.004607\n",
      "Minibatch loss at step 6000: 0.613394\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 85.4%\n",
      "Test accuracy: 90.9%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "hidden_layers_size = 4096\n",
    "\n",
    "learning_starting_rate = 0.1\n",
    "learning_decay_steps = 100\n",
    "learning_decay_rate = 0.95\n",
    "beta = 1*10^-5\n",
    "keep_prob_dropouts=0.7\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    input_weights = tf.Variable(tf.truncated_normal([image_size * image_size, batch_size], mean=0.0, stddev=0.1))\n",
    "    input_biases = tf.Variable(tf.truncated_normal([batch_size]))\n",
    "     \n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([batch_size, hidden_layers_size], mean=0.0, stddev=0.1))\n",
    "    hidden_biases = tf.Variable(tf.truncated_normal([hidden_layers_size]))\n",
    "    \n",
    "    output_weights = tf.Variable(tf.truncated_normal([hidden_layers_size, num_labels], mean=0.0, stddev=0.1))\n",
    "    output_biases = tf.Variable(tf.truncated_normal([num_labels]))\n",
    "\n",
    "    \n",
    "    def network(input): \n",
    "        logit_input = tf.matmul(input, input_weights) + input_biases\n",
    "        logit_hidden = tf.matmul(tf.nn.dropout(tf.nn.relu(logit_input), keep_prob_dropouts), hidden_weights) + hidden_biases    \n",
    "        logit_output = tf.matmul(tf.nn.relu(logit_hidden), output_weights) + output_biases\n",
    "        \n",
    "        return logit_output #+ 0.01*tf.nn.l2_loss(input_weights) + 0.01*tf.nn.l2_loss(hidden_weights) \\\n",
    "                #+ 0.01*tf.nn.l2_loss(output_weights)\n",
    "\n",
    "    # Training computation.\n",
    "    predictions = network(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=predictions))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(learning_starting_rate, global_step, learning_decay_steps, learning_decay_rate)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(predictions)\n",
    "    valid_prediction = tf.nn.softmax(network(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(network(tf_test_dataset))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from IPython import display\n",
    "\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "test_acc_history = []\n",
    "epochs_history = []\n",
    "    \n",
    "num_steps = 6001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Learning rate: %f\" % lr )       \n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            train_acc = accuracy(predictions, batch_labels)\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % train_acc)\n",
    "            val_acc = accuracy(valid_prediction.eval(), valid_labels)\n",
    "            print(\"Validation accuracy: %.1f%%\" % val_acc)\n",
    "            test_acc = accuracy(test_prediction.eval(), test_labels)\n",
    "            \n",
    "            \n",
    "            #display.clear_output(wait=True)            \n",
    "            epochs_history.append(step)\n",
    "            train_acc_history.append(train_acc)\n",
    "            val_acc_history.append(val_acc)\n",
    "            test_acc_history.append(test_acc)\n",
    "            \n",
    "\n",
    "#             plt.plot(epochs_history, train_acc_history, 'r', label='Training accuracy')\n",
    "#             plt.plot(epochs_history, val_acc_history, 'b', label='Validation accuracy')\n",
    "#             plt.plot(epochs_history, test_acc_history, 'g', label='Test accuracy')            \n",
    "#             plt.xlabel('Epochs')\n",
    "#             plt.ylabel('Accuracy')\n",
    "#             plt.legend()\n",
    "            \n",
    "#             plt.show()\n",
    "            #display.display(plt.show())\n",
    "\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
