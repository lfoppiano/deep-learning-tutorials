{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D7tqLMoKF6uq",
        "pycharm": {}
      },
      "source": [
        "Deep Learning\n",
        "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
        "\n",
        "Assignment 5\n",
        "------------\n",
        "\n",
        "The goal of this assignment is to train a Word2Vec skip-gram model over [Text8](http://mattmahoney.net/dc/textdata) data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "both",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "id": "0K1ZyLn04QZf",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "# These are all the modules we\u0027ll be using later. Make sure you can import them\n",
        "# before proceeding further.\n",
        "from __future__ import print_function\n",
        "%matplotlib inline\n",
        "import collections\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "from matplotlib import pylab\n",
        "from six.moves import range\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aCjPJE944bkV",
        "pycharm": {}
      },
      "source": [
        "Download the data from the source website if necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "both",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ]
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 14640,
          "status": "ok",
          "timestamp": 1445964482948,
          "user": {
            "color": "#1FA15D",
            "displayName": "Vincent Vanhoucke",
            "isAnonymous": false,
            "isMe": true,
            "permissionId": "05076109866853157986",
            "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
            "sessionId": "2f1ffade4c9f20de",
            "userId": "102167687554210253930"
          },
          "user_tz": 420
        },
        "id": "RJ-o3UBUFtCw",
        "outputId": "c4ec222c-80b5-4298-e635-93ca9f79c3b7",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Found and verified text8.zip\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": [
        "url \u003d \u0027http://mattmahoney.net/dc/\u0027\n",
        "\n",
        "def maybe_download(filename, expected_bytes):\n",
        "  \"\"\"Download a file if not present, and make sure it\u0027s the right size.\"\"\"\n",
        "  if not os.path.exists(filename):\n",
        "    filename, _ \u003d urlretrieve(url + filename, filename)\n",
        "  statinfo \u003d os.stat(filename)\n",
        "  if statinfo.st_size \u003d\u003d expected_bytes:\n",
        "    print(\u0027Found and verified %s\u0027 % filename)\n",
        "  else:\n",
        "    print(statinfo.st_size)\n",
        "    raise Exception(\n",
        "      \u0027Failed to verify \u0027 + filename + \u0027. Can you get to it with a browser?\u0027)\n",
        "  return filename\n",
        "\n",
        "filename \u003d maybe_download(\u0027text8.zip\u0027, 31344016)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Zqz3XiqI4mZT",
        "pycharm": {}
      },
      "source": [
        "Read the data into a string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "both",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ]
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 28844,
          "status": "ok",
          "timestamp": 1445964497165,
          "user": {
            "color": "#1FA15D",
            "displayName": "Vincent Vanhoucke",
            "isAnonymous": false,
            "isMe": true,
            "permissionId": "05076109866853157986",
            "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
            "sessionId": "2f1ffade4c9f20de",
            "userId": "102167687554210253930"
          },
          "user_tz": 420
        },
        "id": "Mvf09fjugFU_",
        "outputId": "e3a928b4-1645-4fe8-be17-fcf47de5716d",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Data size 17005207\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": [
        "def read_data(filename):\n",
        "  \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
        "  with zipfile.ZipFile(filename) as f:\n",
        "    data \u003d tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
        "  return data\n",
        "  \n",
        "words \u003d read_data(filename)\n",
        "print(\u0027Data size %d\u0027 % len(words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Zdw6i4F8glpp",
        "pycharm": {}
      },
      "source": [
        "Build the dictionary and replace rare words with UNK token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "both",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ]
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 28849,
          "status": "ok",
          "timestamp": 1445964497178,
          "user": {
            "color": "#1FA15D",
            "displayName": "Vincent Vanhoucke",
            "isAnonymous": false,
            "isMe": true,
            "permissionId": "05076109866853157986",
            "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
            "sessionId": "2f1ffade4c9f20de",
            "userId": "102167687554210253930"
          },
          "user_tz": 420
        },
        "id": "gAL1EECXeZsD",
        "outputId": "3fb4ecd1-df67-44b6-a2dc-2291730970b2",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Most common words (+UNK) [[\u0027UNK\u0027, 418391], (\u0027the\u0027, 1061396), (\u0027of\u0027, 593677), (\u0027and\u0027, 416629), (\u0027one\u0027, 411764)]\nSample data [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156]\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": [
        "vocabulary_size \u003d 50000\n",
        "\n",
        "def build_dataset(words):\n",
        "  count \u003d [[\u0027UNK\u0027, -1]]\n",
        "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
        "  dictionary \u003d {}\n",
        "  for word, _ in count:\n",
        "    dictionary[word] \u003d len(dictionary)\n",
        "  data \u003d []\n",
        "  unk_count \u003d 0\n",
        "  for word in words:\n",
        "    if word in dictionary:\n",
        "      index \u003d dictionary[word]\n",
        "    else:\n",
        "      index \u003d 0  # dictionary[\u0027UNK\u0027]\n",
        "      unk_count \u003d unk_count + 1\n",
        "    data.append(index)\n",
        "  count[0][1] \u003d unk_count\n",
        "  reverse_dictionary \u003d dict(zip(dictionary.values(), dictionary.keys())) \n",
        "  return data, count, dictionary, reverse_dictionary\n",
        "\n",
        "data, count, dictionary, reverse_dictionary \u003d build_dataset(words)\n",
        "print(\u0027Most common words (+UNK)\u0027, count[:5])\n",
        "print(\u0027Sample data\u0027, data[:10])\n",
        "del words  # Hint to reduce memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lFwoyygOmWsL",
        "pycharm": {}
      },
      "source": [
        "Function to generate a training batch for the skip-gram model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ]
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 113,
          "status": "ok",
          "timestamp": 1445964901989,
          "user": {
            "color": "#1FA15D",
            "displayName": "Vincent Vanhoucke",
            "isAnonymous": false,
            "isMe": true,
            "permissionId": "05076109866853157986",
            "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
            "sessionId": "2f1ffade4c9f20de",
            "userId": "102167687554210253930"
          },
          "user_tz": 420
        },
        "id": "w9APjA-zmfjV",
        "outputId": "67cccb02-cdaf-4e47-d489-43bcc8d57bb8",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "data_index \u003d 0\n",
        "\n",
        "# skip_window: window where to pick from (window lenght \u003d 2 x skip_window + 1)\n",
        "# num_skip: number of element to pick for each target\n",
        "def generate_batch(batch_size, num_skips, skip_window):\n",
        "  global data_index\n",
        "  assert batch_size % num_skips \u003d\u003d 0\n",
        "  assert num_skips \u003c\u003d 2 * skip_window\n",
        "  batch \u003d np.ndarray(shape\u003d(batch_size), dtype\u003dnp.int32)\n",
        "  labels \u003d np.ndarray(shape\u003d(batch_size, 1), dtype\u003dnp.int32)\n",
        "  span \u003d 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
        "  buffer \u003d collections.deque(maxlen\u003dspan)\n",
        "  # Load element within the window in the buffer near data_index\n",
        "  for _ in range(span):\n",
        "    buffer.append(data[data_index])\n",
        "    data_index \u003d (data_index + 1) % len(data)\n",
        "  for i in range(batch_size // num_skips):\n",
        "    target \u003d skip_window  # target label at the center of the buffer\n",
        "    targets_to_avoid \u003d [ skip_window ]\n",
        "    for j in range(num_skips):\n",
        "      while target in targets_to_avoid:\n",
        "        target \u003d random.randint(0, span - 1)\n",
        "      targets_to_avoid.append(target)\n",
        "      batch[i * num_skips + j] \u003d buffer[skip_window]\n",
        "      labels[i * num_skips + j, 0] \u003d buffer[target]\n",
        "    buffer.append(data[data_index])\n",
        "    data_index \u003d (data_index + 1) % len(data)\n",
        "  return batch, labels\n",
        "\n",
        "print(\u0027data:\u0027, [reverse_dictionary[di] for di in data[:8]])\n",
        "\n",
        "for num_skips, skip_window in [(2, 1), (4, 2)]:\n",
        "    data_index \u003d 0\n",
        "    batch, labels \u003d generate_batch(batch_size\u003d8, num_skips\u003dnum_skips, skip_window\u003dskip_window)\n",
        "    print(\u0027\\nwith num_skips \u003d %d and skip_window \u003d %d:\u0027 % (num_skips, skip_window))\n",
        "    print(\u0027    batch:\u0027, [reverse_dictionary[bi] for bi in batch])\n",
        "    print(\u0027    labels:\u0027, [reverse_dictionary[li] for li in labels.reshape(8)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ofd1MbBuwiva",
        "pycharm": {}
      },
      "source": [
        "Train a skip-gram model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "id": "8pQKsV4Vwlzy",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "batch_size \u003d 128\n",
        "embedding_size \u003d 128 # Dimension of the embedding vector.\n",
        "skip_window \u003d 1 # How many words to consider left and right.\n",
        "num_skips \u003d 2 # How many times to reuse an input to generate a label.\n",
        "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
        "# validation samples to the words that have a low numeric ID, which by\n",
        "# construction are also the most frequent. \n",
        "valid_size \u003d 16 # Random set of words to evaluate similarity on.\n",
        "valid_window \u003d 100 # Only pick dev samples in the head of the distribution.\n",
        "valid_examples \u003d np.array(random.sample(range(valid_window), valid_size))\n",
        "num_sampled \u003d 64 # Number of negative examples to sample.\n",
        "\n",
        "graph \u003d tf.Graph()\n",
        "\n",
        "with graph.as_default(): #, tf.device(\u0027/cpu:0\u0027):\n",
        "\n",
        "  # Input data.\n",
        "  train_dataset \u003d tf.placeholder(tf.int32, shape\u003d[batch_size])\n",
        "  train_labels \u003d tf.placeholder(tf.int32, shape\u003d[batch_size, 1])\n",
        "  valid_dataset \u003d tf.constant(valid_examples, dtype\u003dtf.int32)\n",
        "  \n",
        "  # Variables.\n",
        "  embeddings \u003d tf.Variable(\n",
        "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
        "  softmax_weights \u003d tf.Variable(\n",
        "    tf.truncated_normal([vocabulary_size, embedding_size],\n",
        "                         stddev\u003d1.0 / math.sqrt(embedding_size)))\n",
        "  softmax_biases \u003d tf.Variable(tf.zeros([vocabulary_size]))\n",
        "  \n",
        "  # Model.\n",
        "  # Look up embeddings for inputs.\n",
        "  embed \u003d tf.nn.embedding_lookup(embeddings, train_dataset) \n",
        "  print(embed)\n",
        "  # Compute the softmax loss, using a sample of the negative labels each time.\n",
        "  loss \u003d tf.reduce_mean(\n",
        "    tf.nn.sampled_softmax_loss(weights\u003dsoftmax_weights, biases\u003dsoftmax_biases, inputs\u003dembed,\n",
        "                               labels\u003dtrain_labels, num_sampled\u003dnum_sampled, num_classes\u003dvocabulary_size))\n",
        "\n",
        "  # Optimizer.\n",
        "  # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n",
        "  # This is because the embeddings are defined as a variable quantity and the\n",
        "  # optimizer\u0027s `minimize` method will by default modify all variable quantities \n",
        "  # that contribute to the tensor it is passed.\n",
        "  # See docs on `tf.train.Optimizer.minimize()` for more details.\n",
        "  optimizer \u003d tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
        "  \n",
        "  # Compute the similarity between minibatch examples and all embeddings.\n",
        "  # We use the cosine distance:\n",
        "  norm \u003d tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims\u003dTrue))\n",
        "  normalized_embeddings \u003d embeddings / norm\n",
        "  valid_embeddings \u003d tf.nn.embedding_lookup(\n",
        "    normalized_embeddings, valid_dataset)\n",
        "  similarity \u003d tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 23
            },
            {
              "item_id": 48
            },
            {
              "item_id": 61
            }
          ]
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 436189,
          "status": "ok",
          "timestamp": 1445965429787,
          "user": {
            "color": "#1FA15D",
            "displayName": "Vincent Vanhoucke",
            "isAnonymous": false,
            "isMe": true,
            "permissionId": "05076109866853157986",
            "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
            "sessionId": "2f1ffade4c9f20de",
            "userId": "102167687554210253930"
          },
          "user_tz": 420
        },
        "id": "1bQFGceBxrWW",
        "outputId": "5ebd6d9a-33c6-4bcd-bf6d-252b0b6055e4",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "num_steps \u003d 100001\n",
        "\n",
        "with tf.Session(graph\u003dgraph) as session:\n",
        "  tf.global_variables_initializer().run()\n",
        "  print(\u0027Initialized\u0027)\n",
        "  average_loss \u003d 0\n",
        "  for step in range(num_steps):\n",
        "    batch_data, batch_labels \u003d generate_batch(batch_size, num_skips, skip_window)\n",
        "    feed_dict \u003d {train_dataset : batch_data, train_labels : batch_labels}\n",
        "    _, l \u003d session.run([optimizer, loss], feed_dict\u003dfeed_dict)\n",
        "    average_loss +\u003d l\n",
        "    if step % 2000 \u003d\u003d 0:\n",
        "      if step \u003e 0:\n",
        "        average_loss \u003d average_loss / 2000\n",
        "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
        "      print(\u0027Average loss at step %d: %f\u0027 % (step, average_loss))\n",
        "      average_loss \u003d 0\n",
        "    # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
        "    if step % 10000 \u003d\u003d 0:\n",
        "      sim \u003d similarity.eval()\n",
        "      for i in range(valid_size):\n",
        "        valid_word \u003d reverse_dictionary[valid_examples[i]]\n",
        "        top_k \u003d 8 # number of nearest neighbors\n",
        "        nearest \u003d (-sim[i, :]).argsort()[1:top_k+1]\n",
        "        log \u003d \u0027Nearest to %s:\u0027 % valid_word\n",
        "        for k in range(top_k):\n",
        "          close_word \u003d reverse_dictionary[nearest[k]]\n",
        "          log \u003d \u0027%s %s,\u0027 % (log, close_word)\n",
        "        print(log)\n",
        "  final_embeddings \u003d normalized_embeddings.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "id": "jjJXYA_XzV79",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "num_points \u003d 200\n",
        "\n",
        "tsne \u003d TSNE(perplexity\u003d30, n_components\u003d2, init\u003d\u0027pca\u0027, n_iter\u003d5000, method\u003d\u0027exact\u0027)\n",
        "two_d_embeddings \u003d tsne.fit_transform(final_embeddings[1:num_points+1, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ]
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 4763,
          "status": "ok",
          "timestamp": 1445965465525,
          "user": {
            "color": "#1FA15D",
            "displayName": "Vincent Vanhoucke",
            "isAnonymous": false,
            "isMe": true,
            "permissionId": "05076109866853157986",
            "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
            "sessionId": "2f1ffade4c9f20de",
            "userId": "102167687554210253930"
          },
          "user_tz": 420
        },
        "id": "o_e0D_UezcDe",
        "outputId": "df22e4a5-e8ec-4e5e-d384-c6cf37c68c34",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def plot(embeddings, labels):\n",
        "  assert embeddings.shape[0] \u003e\u003d len(labels), \u0027More labels than embeddings\u0027\n",
        "  pylab.figure(figsize\u003d(15,15))  # in inches\n",
        "  for i, label in enumerate(labels):\n",
        "    x, y \u003d embeddings[i,:]\n",
        "    pylab.scatter(x, y)\n",
        "    pylab.annotate(label, xy\u003d(x, y), xytext\u003d(5, 2), textcoords\u003d\u0027offset points\u0027,\n",
        "                   ha\u003d\u0027right\u0027, va\u003d\u0027bottom\u0027)\n",
        "  pylab.show()\n",
        "\n",
        "words \u003d [reverse_dictionary[i] for i in range(1, num_points+1)]\n",
        "plot(two_d_embeddings, words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QB5EFrBnpNnc",
        "pycharm": {}
      },
      "source": [
        "---\n",
        "\n",
        "Problem\n",
        "-------\n",
        "\n",
        "An alternative to skip-gram is another Word2Vec model called [CBOW](http://arxiv.org/abs/1301.3781) (Continuous Bag of Words). In the CBOW model, instead of predicting a context word from a word vector, you predict a word from the sum of all the word vectors in its context. Implement and evaluate a CBOW model trained on the text8 dataset.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "\nwith context_window \u003d 1:\n    batch: [[\u0027anarchism\u0027, \u0027as\u0027], [\u0027originated\u0027, \u0027a\u0027], [\u0027as\u0027, \u0027term\u0027], [\u0027a\u0027, \u0027of\u0027], [\u0027term\u0027, \u0027abuse\u0027], [\u0027of\u0027, \u0027first\u0027], [\u0027abuse\u0027, \u0027used\u0027], [\u0027first\u0027, \u0027against\u0027], [\u0027used\u0027, \u0027early\u0027], [\u0027against\u0027, \u0027working\u0027]]\n    labels: [\u0027originated\u0027, \u0027as\u0027, \u0027a\u0027, \u0027term\u0027, \u0027of\u0027, \u0027abuse\u0027, \u0027first\u0027, \u0027used\u0027, \u0027against\u0027, \u0027early\u0027]\n\nwith context_window \u003d 2:\n    batch: [[\u0027anarchism\u0027, \u0027originated\u0027, \u0027a\u0027, \u0027term\u0027], [\u0027originated\u0027, \u0027as\u0027, \u0027term\u0027, \u0027of\u0027], [\u0027as\u0027, \u0027a\u0027, \u0027of\u0027, \u0027abuse\u0027], [\u0027a\u0027, \u0027term\u0027, \u0027abuse\u0027, \u0027first\u0027], [\u0027term\u0027, \u0027of\u0027, \u0027first\u0027, \u0027used\u0027], [\u0027of\u0027, \u0027abuse\u0027, \u0027used\u0027, \u0027against\u0027], [\u0027abuse\u0027, \u0027first\u0027, \u0027against\u0027, \u0027early\u0027], [\u0027first\u0027, \u0027used\u0027, \u0027early\u0027, \u0027working\u0027], [\u0027used\u0027, \u0027against\u0027, \u0027working\u0027, \u0027class\u0027], [\u0027against\u0027, \u0027early\u0027, \u0027class\u0027, \u0027radicals\u0027]]\n    labels: [\u0027as\u0027, \u0027a\u0027, \u0027term\u0027, \u0027of\u0027, \u0027abuse\u0027, \u0027first\u0027, \u0027used\u0027, \u0027against\u0027, \u0027early\u0027, \u0027working\u0027]\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "data_index \u003d 0\n\n# skip_window: window where to pick from (window lenght \u003d 2 x skip_window + 1)\n# num_skip: number of element to pick for each target\ndef generate_batch_cbow(batch_size, context_window):\n  global data_index\n  batch \u003d np.ndarray(shape\u003d(batch_size, 2 * context_window), dtype\u003dnp.int32)\n  labels \u003d np.ndarray(shape\u003d(batch_size, 1), dtype\u003dnp.int32)\n  span \u003d 2 * context_window + 1 # [ skip_window target skip_window ]\n  buffer \u003d collections.deque(maxlen\u003dspan)\n\n  # Load element within the window in the buffer near data_index\n  for _ in range(span):\n    buffer.append(data[data_index])\n    data_index \u003d (data_index + 1) % len(data)\n    \n  for i  in range(batch_size):\n    context_value \u003d []\n    target \u003d context_window  # target label at the center of the buffer\n    for idx, val in enumerate(buffer): \n        if idx is not target:  \n            context_value.append(val)\n            \n    batch[i] \u003d context_value\n    labels[i, 0] \u003d buffer[target]\n    buffer.append(data[data_index])\n    data_index \u003d (data_index + 1) % len(data)\n\n  return batch,labels\n\n# data_index \u003d 6781\n# \n# batch, label  \u003dgenerate_batch_cbow(128, 1)\n# print(\u0027    batch:\u0027, [[reverse_dictionary[b] for b in bi] for bi in batch])\n# print(\u0027    labels:\u0027, [reverse_dictionary[li] for li in labels.reshape(128)]) \n\n\nfor context_window in [1, 2]:\n    data_index \u003d 0\n    batch, labels \u003d generate_batch_cbow(batch_size\u003d10, context_window\u003dcontext_window)\n    print(\u0027\\nwith context_window \u003d %d:\u0027 % (context_window))\n    print(\u0027    batch:\u0027, [[reverse_dictionary[b] for b in bi] for bi in batch])\n    print(\u0027    labels:\u0027, [reverse_dictionary[li] for li in labels.reshape(10)]) \n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:1124: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nCreate a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": [
        "batch_size \u003d 128\n",
        "embedding_size \u003d 128 # Dimension of the embedding vector.\n",
        "context_window \u003d 1\n",
        "\n",
        "valid_size \u003d 16 # Random set of words to evaluate similarity on.\n",
        "valid_window \u003d 100 # Only pick dev samples in the head of the distribution.\n",
        "valid_examples \u003d np.array(random.sample(range(valid_window), valid_size))\n",
        "num_sampled \u003d 64 # Number of negative examples to sample.\n",
        "\n",
        "graph \u003d tf.Graph()\n",
        "\n",
        "with graph.as_default(): #, tf.device(\u0027/cpu:0\u0027):\n",
        "\n",
        "  # Input data.\n",
        "  train_dataset \u003d tf.placeholder(tf.int32, shape\u003d[batch_size, 2 * context_window])\n",
        "  train_labels \u003d tf.placeholder(tf.int32, shape\u003d[batch_size, 1])\n",
        "  valid_dataset \u003d tf.constant(valid_examples, dtype\u003dtf.int32)\n",
        "  \n",
        "  # Variables\n",
        "  embeddings \u003d tf.Variable(\n",
        "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
        "  softmax_weights \u003d tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
        "                         stddev\u003d1.0 / math.sqrt(embedding_size)))\n",
        "  softmax_biases \u003d tf.Variable(tf.zeros([vocabulary_size]))\n",
        "  \n",
        "  # Model.\n",
        "  # Look up embeddings for inputs.\n",
        "  embed \u003d tf.nn.embedding_lookup(embeddings, train_dataset[:,0])\n",
        "  for idx in range(1, 2 * context_window): \n",
        "    embed +\u003d tf.nn.embedding_lookup(embeddings, train_dataset[:,idx])\n",
        "\n",
        "    \n",
        "#   embed \u003d tf.nn.embedding_lookup(embeddings, train_dataset)\n",
        "  # Compute the softmax loss, using a sample of the negative labels each time.\n",
        "  loss \u003d tf.reduce_mean(\n",
        "    tf.nn.sampled_softmax_loss(weights\u003dsoftmax_weights, biases\u003dsoftmax_biases, inputs\u003dembed,\n",
        "                               labels\u003dtrain_labels, num_sampled\u003dnum_sampled, num_classes\u003dvocabulary_size))\n",
        "\n",
        "  # Optimizer.\n",
        "  # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n",
        "  # This is because the embeddings are defined as a variable quantity and the\n",
        "  # optimizer\u0027s `minimize` method will by default modify all variable quantities \n",
        "  # that contribute to the tensor it is passed.\n",
        "  # See docs on `tf.train.Optimizer.minimize()` for more details.\n",
        "  optimizer \u003d tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
        "  \n",
        "  # Compute the similarity between minibatch examples and all embeddings.\n",
        "  # We use the cosine distance:\n",
        "  norm \u003d tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims\u003dTrue))\n",
        "  normalized_embeddings \u003d embeddings / norm\n",
        "  valid_embeddings \u003d tf.nn.embedding_lookup(\n",
        "    normalized_embeddings, valid_dataset)\n",
        "  similarity \u003d tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "1\n",
            "Initialized\n",
            "Average loss at step 0: 7.661128\nNearest to system: taxing, griffiths, counterrevolutionary, lincoln, pointer, burke, monogamous, fac,\nNearest to as: clandestine, angelo, andrey, matthias, rodeo, smiths, iso, personally,\nNearest to this: resemblances, idle, location, duly, surge, mondays, forecast, reflections,\nNearest to only: unam, inspirations, nominees, bskyb, dialling, arbitrate, washtenaw, catwoman,\nNearest to also: sept, nihonshoki, wp, regulator, armenian, sahih, protein, segment,\nNearest to it: outgoing, dust, tibeto, blanca, burden, emirs, mt, instruction,\nNearest to most: imprison, adage, tendencies, machen, playful, waterborne, signage, kolmogorov,\nNearest to was: burying, provocation, gatehouse, disruptive, innovators, devastates, olympia, oeis,\nNearest to war: fn, paulist, preseason, hurdler, rattle, faber, larsen, aries,\nNearest to b: cryopreserved, tls, carve, republika, colloquial, soda, combine, encircle,\nNearest to when: snapshot, louisiana, catharine, refutation, nullify, no, chickenpox, alai,\nNearest to eight: memorabilia, nisan, wandering, bees, continuing, powdered, naturalization, convocation,\nNearest to be: mentha, pauper, symbolizing, belle, cobalt, chartered, apprehension, sudanese,\nNearest to for: under, emporia, wikibook, giambattista, override, berenguer, santiago, taunus,\nNearest to many: linus, lapsed, fein, frei, malay, bullshit, leinster, crops,\nNearest to into: aloud, je, hopeful, herbert, preponderance, ahaziah, mt, alhazen,\n",
            "Average loss at step 2000: 4.095984\n",
            "Average loss at step 4000: 3.563854\n",
            "Average loss at step 6000: 3.387347\n",
            "Average loss at step 8000: 3.233143\n",
            "Average loss at step 10000: 3.173667\nNearest to system: griffiths, mondays, observation, linguistic, lincoln, moons, rf, kirlian,\nNearest to as: steamed, pala, treatises, tld, basica, femininity, maeterlinck, for,\nNearest to this: it, which, the, homogenized, his, that, idle, kev,\nNearest to only: pores, avl, inspirations, lovers, antigen, enrico, xerxes, prc,\nNearest to also: which, sometimes, often, now, not, never, generally, officially,\nNearest to it: he, she, this, there, they, which, homogenized, that,\nNearest to most: some, more, adage, waterborne, clinically, imprison, prescription, romanization,\nNearest to was: is, has, were, had, became, be, eberhard, kidnappings,\nNearest to war: cia, hurdler, paulist, fn, appellations, capitalisation, ticino, preseason,\nNearest to b: d, expectation, chariot, pentecost, weather, ncp, republika, peroxides,\nNearest to when: catharine, asch, after, louisiana, colloidal, that, before, during,\nNearest to eight: nine, six, seven, four, five, three, zero, two,\nNearest to be: have, backfire, been, were, is, wallet, was, graffiti,\nNearest to for: under, hippos, of, with, minogue, after, by, as,\nNearest to many: some, several, plutocracy, these, crops, specialize, all, luca,\nNearest to into: from, ossie, glacier, in, between, ragged, ulyanov, tui,\n",
            "Average loss at step 12000: 3.205959\n",
            "Average loss at step 14000: 3.148568\n",
            "Average loss at step 16000: 3.172344\n",
            "Average loss at step 18000: 3.129505\n",
            "Average loss at step 20000: 2.997384\nNearest to system: systems, observation, tax, visas, mondays, moons, islet, electronically,\nNearest to as: discernible, steamed, ceased, gigahertz, yama, tld, tetrahedra, bechtel,\nNearest to this: which, it, that, homogenized, the, forecast, baggage, some,\nNearest to only: pores, ker, executions, adlai, first, perhaps, placed, bawerk,\nNearest to also: often, sometimes, now, which, generally, still, below, never,\nNearest to it: he, she, this, there, still, they, which, jaina,\nNearest to most: more, some, avram, adage, clinically, porting, many, less,\nNearest to was: is, has, were, became, had, been, entailed, does,\nNearest to war: cia, yields, sparrow, unspecified, appellations, hurdler, contrapuntal, approximants,\nNearest to b: d, republika, clocking, henrik, expectation, chariot, weather, pentecost,\nNearest to when: before, if, after, where, while, ticker, assertive, until,\nNearest to eight: six, seven, nine, four, five, three, zero, two,\nNearest to be: have, been, refer, being, backfire, create, is, condita,\nNearest to for: commended, hippos, berenguer, featureless, of, spoiling, lags, ellipsis,\nNearest to many: some, several, these, various, plutocracy, pork, such, those,\nNearest to into: from, between, back, without, ossie, glacier, through, aloud,\n",
            "Average loss at step 22000: 3.077211\n",
            "Average loss at step 24000: 3.033839\n",
            "Average loss at step 26000: 2.997984\n",
            "Average loss at step 28000: 3.026425\n",
            "Average loss at step 30000: 3.001304\nNearest to system: systems, moons, origin, parasites, griffiths, group, applications, islet,\nNearest to as: peyton, when, steamed, in, pala, adventurers, maeterlinck, nva,\nNearest to this: which, it, the, homogenized, carte, that, forecast, parsecs,\nNearest to only: adlai, aqua, pores, first, capitalism, best, phonemes, executions,\nNearest to also: now, still, often, sometimes, never, actually, generally, which,\nNearest to it: he, she, this, there, jaina, still, they, screenshot,\nNearest to most: some, more, many, clinically, use, avram, particularly, less,\nNearest to was: is, had, became, has, were, being, be, remains,\nNearest to war: yields, cia, wars, hurdler, unspecified, contrapuntal, sparrow, electrostatics,\nNearest to b: d, pentecost, n, henrik, f, clocking, shuja, solitary,\nNearest to when: if, before, while, after, where, however, because, during,\nNearest to eight: nine, seven, six, four, five, three, zero, two,\nNearest to be: been, being, backfire, have, refer, become, is, was,\nNearest to for: walser, berenguer, in, hippos, cri, extraterrestrials, without, blyton,\nNearest to many: some, several, various, these, most, both, such, few,\nNearest to into: from, through, back, within, around, without, in, ossie,\n",
            "Average loss at step 32000: 2.827117\n",
            "Average loss at step 34000: 2.951507\n",
            "Average loss at step 36000: 2.942908\n",
            "Average loss at step 38000: 2.945647\n",
            "Average loss at step 40000: 2.928449\nNearest to system: systems, technical, canteen, moons, ibrd, mondays, bayreuth, origin,\nNearest to as: maeterlinck, pala, steamed, discernible, gigahertz, when, became, undergrowth,\nNearest to this: which, it, what, another, plazas, that, homogenized, the,\nNearest to only: best, always, adlai, perhaps, aristophanes, achievable, asshole, softer,\nNearest to also: now, still, often, sometimes, never, generally, actually, below,\nNearest to it: he, she, there, this, they, jaina, still, arcturus,\nNearest to most: more, some, less, many, particularly, avram, logarithmically, very,\nNearest to was: is, became, had, were, has, been, remains, becomes,\nNearest to war: wars, contrapuntal, yields, sparrow, franke, cia, capitalisation, justifying,\nNearest to b: d, henrik, n, c, f, hertz, clocking, eminently,\nNearest to when: if, before, after, while, during, although, however, where,\nNearest to eight: nine, seven, six, five, four, three, zero, two,\nNearest to be: been, being, have, refer, become, backfire, create, knicks,\nNearest to for: when, berenguer, extraterrestrials, without, hippos, while, godlike, alongside,\nNearest to many: some, several, various, most, numerous, these, all, those,\nNearest to into: through, from, back, within, under, between, globin, maaouya,\n",
            "Average loss at step 42000: 2.946862\n",
            "Average loss at step 44000: 2.951919\n",
            "Average loss at step 46000: 2.904313\n",
            "Average loss at step 48000: 2.866449\n",
            "Average loss at step 50000: 2.848047\nNearest to system: systems, moons, canteen, group, structure, companion, type, healing,\nNearest to as: like, discernible, connectives, curving, pala, maeterlinck, steamed, gigahertz,\nNearest to this: it, which, kev, itself, diocletian, what, the, some,\nNearest to only: even, no, banna, adlai, actually, perhaps, adventurers, geddy,\nNearest to also: now, still, often, never, actually, sometimes, which, generally,\nNearest to it: he, she, this, there, they, jaina, which, however,\nNearest to most: more, some, less, particularly, many, especially, avram, xinjiang,\nNearest to was: is, became, had, been, were, has, remains, isfahan,\nNearest to war: wars, contrapuntal, cia, ovulation, yields, unspecified, battle, justifying,\nNearest to b: d, actor, daniel, lucy, c, pentecost, pedagogue, zeke,\nNearest to when: if, before, while, after, where, during, though, until,\nNearest to eight: nine, seven, six, four, five, three, zero, two,\nNearest to be: have, been, refer, become, being, backfire, is, contribute,\nNearest to for: walser, berenguer, woodcuts, including, damper, when, steinberg, commended,\nNearest to many: several, some, various, these, all, such, numerous, certain,\nNearest to into: through, from, within, back, out, between, under, around,\n",
            "Average loss at step 52000: 2.885094\n",
            "Average loss at step 54000: 2.862266\n",
            "Average loss at step 56000: 2.857399\n",
            "Average loss at step 58000: 2.749002\n",
            "Average loss at step 60000: 2.832772\nNearest to system: systems, structure, companion, term, group, xs, liguria, moons,\nNearest to as: maeterlinck, northernmost, pala, collects, how, doctoral, adar, tld,\nNearest to this: it, which, the, scriptural, what, plazas, kev, some,\nNearest to only: suffice, actually, usually, champlain, weave, always, achievable, no,\nNearest to also: still, often, now, actually, never, sometimes, generally, usually,\nNearest to it: he, she, there, this, they, itself, jaina, arcturus,\nNearest to most: more, some, many, particularly, less, because, especially, shreveport,\nNearest to was: is, had, became, has, were, becomes, remained, been,\nNearest to war: wars, cia, discipline, contrapuntal, yields, sanhedrin, seabirds, thermotropic,\nNearest to b: d, c, restraints, yams, x, lucy, rejecting, palin,\nNearest to when: if, while, before, where, after, during, although, though,\nNearest to eight: nine, seven, six, five, four, three, two, one,\nNearest to be: been, become, have, being, refer, backfire, provide, is,\nNearest to for: without, berenguer, walser, hippos, timeless, extraterrestrials, after, epsom,\nNearest to many: several, some, numerous, these, various, all, few, certain,\nNearest to into: through, from, back, within, under, between, pontificia, globin,\n",
            "Average loss at step 62000: 2.831603\n",
            "Average loss at step 64000: 2.774314\n",
            "Average loss at step 66000: 2.763639\n",
            "Average loss at step 68000: 2.706818\n",
            "Average loss at step 70000: 2.785653\nNearest to system: systems, structure, group, workloads, applications, format, process, program,\nNearest to as: maeterlinck, pohjola, when, yonkers, discernible, anarcho, attributable, trotskyist,\nNearest to this: which, it, what, plazas, quadratic, another, the, some,\nNearest to only: both, either, best, actually, always, perhaps, phonemes, suffice,\nNearest to also: actually, still, now, often, never, usually, generally, typically,\nNearest to it: she, he, there, this, they, jaina, unclear, but,\nNearest to most: less, more, particularly, some, many, especially, hugely, convocation,\nNearest to was: is, had, became, were, has, becomes, did, been,\nNearest to war: wars, cia, discipline, thermotropic, yields, sanhedrin, dinoflagellates, contrapuntal,\nNearest to b: d, actor, balloon, waned, pedagogue, zeke, mindstorms, paratroopers,\nNearest to when: if, before, while, after, although, where, though, during,\nNearest to eight: seven, nine, six, five, four, three, zero, two,\nNearest to be: been, become, have, refer, being, is, backfire, add,\nNearest to for: after, against, when, berenguer, alongside, in, of, without,\nNearest to many: several, some, numerous, various, these, all, few, most,\nNearest to into: through, from, within, back, under, pontificia, strategists, waals,\n",
            "Average loss at step 72000: 2.792933\n",
            "Average loss at step 74000: 2.642873\n",
            "Average loss at step 76000: 2.805455\n",
            "Average loss at step 78000: 2.819283\n",
            "Average loss at step 80000: 2.783291\nNearest to system: systems, group, format, satire, applications, structure, hitpa, program,\nNearest to as: motorsport, curving, maeterlinck, discernible, peyton, adar, narita, like,\nNearest to this: which, it, plazas, what, unn, algardi, another, that,\nNearest to only: suffice, either, actually, always, aft, even, no, adlai,\nNearest to also: generally, often, typically, now, sometimes, actually, never, still,\nNearest to it: he, she, there, this, they, arcturus, itself, we,\nNearest to most: more, less, some, particularly, many, use, clinically, especially,\nNearest to was: is, became, has, were, had, becomes, been, remained,\nNearest to war: wars, cia, discipline, clare, justifying, thermotropic, contrapuntal, refugees,\nNearest to b: d, c, pedagogue, r, h, stephen, voles, zeke,\nNearest to when: if, while, although, before, after, during, though, where,\nNearest to eight: nine, six, seven, five, four, three, zero, two,\nNearest to be: become, been, refer, being, backfire, was, add, easily,\nNearest to for: when, after, including, without, while, during, against, alongside,\nNearest to many: several, some, numerous, various, all, few, most, these,\nNearest to into: through, within, from, back, across, around, waals, pontificia,\n",
            "Average loss at step 82000: 2.666211\n",
            "Average loss at step 84000: 2.768265\n",
            "Average loss at step 86000: 2.733771\n",
            "Average loss at step 88000: 2.754061\n",
            "Average loss at step 90000: 2.734672\nNearest to system: systems, applications, satire, quadrivium, eater, hitpa, group, jingles,\nNearest to as: including, northernmost, adar, balloon, peyton, statesman, maeterlinck, motorsport,\nNearest to this: which, it, what, plazas, unn, some, each, unintelligible,\nNearest to only: either, always, champlain, suffice, perhaps, until, achievable, last,\nNearest to also: still, often, now, never, actually, sometimes, typically, generally,\nNearest to it: he, she, there, this, arcturus, jaina, they, itself,\nNearest to most: more, some, many, particularly, clinically, less, especially, hugely,\nNearest to was: is, became, has, had, were, becomes, remained, seems,\nNearest to war: wars, justifying, cia, unspecified, sparrow, franke, discipline, thermotropic,\nNearest to b: d, c, f, pedagogue, earls, j, x, e,\nNearest to when: if, although, while, before, though, after, where, because,\nNearest to eight: nine, six, seven, four, five, three, zero, two,\nNearest to be: become, been, being, refer, backfire, have, easily, adopt,\nNearest to for: berenguer, dental, after, including, exemplify, despite, walser, of,\nNearest to many: some, several, numerous, both, various, few, these, all,\nNearest to into: through, within, from, back, off, under, pontificia, onto,\n",
            "Average loss at step 92000: 2.678123\n",
            "Average loss at step 94000: 2.726084\n",
            "Average loss at step 96000: 2.700942\n",
            "Average loss at step 98000: 2.366557\n",
            "Average loss at step 100000: 2.402054\nNearest to system: systems, network, applications, procedure, jingles, type, unit, bayreuth,\nNearest to as: maeterlinck, psycho, before, after, yonkers, for, steamed, motorsport,\nNearest to this: some, which, it, another, plazas, what, his, each,\nNearest to only: either, last, suffice, until, adlai, geddy, prosecutions, best,\nNearest to also: actually, still, often, now, never, sometimes, typically, currently,\nNearest to it: he, she, there, itself, this, arcturus, they, jaina,\nNearest to most: more, less, especially, particularly, some, clinically, hugely, many,\nNearest to was: is, became, had, has, were, remains, been, seems,\nNearest to war: wars, justifying, savages, bengals, racine, cia, unspecified, sparrow,\nNearest to b: d, zeke, yams, waned, cerium, celia, infiltration, hanford,\nNearest to when: if, before, though, while, although, where, after, because,\nNearest to eight: nine, seven, six, five, four, three, zero, two,\nNearest to be: become, been, being, have, backfire, refer, remain, were,\nNearest to for: after, walser, during, from, when, without, before, as,\nNearest to many: several, some, numerous, various, few, these, those, such,\nNearest to into: through, back, under, within, from, onto, off, down,\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": [
        "num_steps \u003d 100001\n",
        "print(context_window)\n",
        "\n",
        "with tf.Session(graph\u003dgraph) as session:\n",
        "  tf.global_variables_initializer().run()\n",
        "  print(\u0027Initialized\u0027)\n",
        "  average_loss \u003d 0\n",
        "  for step in range(num_steps):\n",
        "    batch_data, batch_labels \u003d generate_batch_cbow(batch_size, context_window)\n",
        "    \n",
        "    feed_dict \u003d {train_dataset : batch_data, train_labels : batch_labels}\n",
        "    _, l \u003d session.run([optimizer, loss], feed_dict\u003dfeed_dict)\n",
        "    average_loss +\u003d l\n",
        "    if step % 2000 \u003d\u003d 0:\n",
        "      if step \u003e 0:\n",
        "        average_loss \u003d average_loss / 2000\n",
        "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
        "      print(\u0027Average loss at step %d: %f\u0027 % (step, average_loss))\n",
        "      average_loss \u003d 0\n",
        "    # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
        "    if step % 10000 \u003d\u003d 0:\n",
        "      sim \u003d similarity.eval()\n",
        "      for i in range(valid_size):\n",
        "        valid_word \u003d reverse_dictionary[valid_examples[i]]\n",
        "        top_k \u003d 8 # number of nearest neighbors\n",
        "        nearest \u003d (-sim[i, :]).argsort()[1:top_k+1]\n",
        "        log \u003d \u0027Nearest to %s:\u0027 % valid_word\n",
        "        for k in range(top_k):\n",
        "          close_word \u003d reverse_dictionary[nearest[k]]\n",
        "          log \u003d \u0027%s %s,\u0027 % (log, close_word)\n",
        "        print(log)\n",
        "  final_embeddings \u003d normalized_embeddings.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-9-4a578711b1bb\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtsne\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;34m\u0027pca\u0027\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;34m\u0027exact\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 4\u001b[0;31m \u001b[0mtwo_d_embeddings\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mtsne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_points\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name \u0027final_embeddings\u0027 is not defined"
          ],
          "ename": "NameError",
          "evalue": "name \u0027final_embeddings\u0027 is not defined",
          "output_type": "error"
        }
      ],
      "source": [
        "num_points \u003d 200\n",
        "\n",
        "tsne \u003d TSNE(perplexity\u003d30, n_components\u003d2, init\u003d\u0027pca\u0027, n_iter\u003d5000, method\u003d\u0027exact\u0027)\n",
        "two_d_embeddings \u003d tsne.fit_transform(final_embeddings[1:num_points+1, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-10-25a2847bf356\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mreverse_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_points\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 12\u001b[0;31m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwo_d_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name \u0027two_d_embeddings\u0027 is not defined"
          ],
          "ename": "NameError",
          "evalue": "name \u0027two_d_embeddings\u0027 is not defined",
          "output_type": "error"
        }
      ],
      "source": [
        "def plot(embeddings, labels):\n",
        "  assert embeddings.shape[0] \u003e\u003d len(labels), \u0027More labels than embeddings\u0027\n",
        "  pylab.figure(figsize\u003d(15,15))  # in inches\n",
        "  for i, label in enumerate(labels):\n",
        "    x, y \u003d embeddings[i,:]\n",
        "    pylab.scatter(x, y)\n",
        "    pylab.annotate(label, xy\u003d(x, y), xytext\u003d(5, 2), textcoords\u003d\u0027offset points\u0027,\n",
        "                   ha\u003d\u0027right\u0027, va\u003d\u0027bottom\u0027)\n",
        "  pylab.show()\n",
        "\n",
        "words \u003d [reverse_dictionary[i] for i in range(1, num_points+1)]\n",
        "plot(two_d_embeddings, words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "default_view": {},
      "name": "5_word2vec.ipynb",
      "provenance": [],
      "version": "0.3.2",
      "views": {}
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}